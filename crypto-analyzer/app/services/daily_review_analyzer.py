#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¯æ—¥å¤ç›˜åˆ†æå™¨ - Daily Review Analyzer

åŠŸèƒ½:
1. æ¯å¤©å¤ç›˜24Hè¡Œæƒ…èµ°åŠ¿
2. è¯†åˆ«é”™è¿‡çš„å¤§è¡Œæƒ…æœºä¼š
3. åˆ†æç°æœ‰ä¿¡å·æ•æ‰æ•ˆæœ
4. è‡ªåŠ¨ä¼˜åŒ–ä¿¡å·å‚æ•°
5. ç”Ÿæˆå¤ç›˜æŠ¥å‘Š

Author: Claude
Date: 2026-01-26
"""

import asyncio
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
from decimal import Decimal
from loguru import logger
import pymysql
import json
from dataclasses import dataclass, asdict


@dataclass
class BigMoveOpportunity:
    """å¤§è¡Œæƒ…æœºä¼š"""
    symbol: str
    start_time: datetime
    end_time: datetime
    timeframe: str  # '5m', '15m', '1h'
    move_type: str  # 'pump' (ä¸Šæ¶¨), 'dump' (ä¸‹è·Œ)
    price_change_pct: float  # ä»·æ ¼å˜åŒ–ç™¾åˆ†æ¯”
    volume_ratio: float  # æˆäº¤é‡å€æ•°
    max_price: float
    min_price: float
    start_price: float
    end_price: float

    # ä¿¡å·æ•æ‰æƒ…å†µ
    captured: bool  # æ˜¯å¦æ•æ‰åˆ°
    capture_delay_minutes: Optional[int]  # æ•æ‰å»¶è¿Ÿ(åˆ†é’Ÿ)
    signal_type: Optional[str]  # æ•æ‰çš„ä¿¡å·ç±»å‹
    position_pnl_pct: Optional[float]  # å®é™…ç›ˆäºç™¾åˆ†æ¯”

    # é”™è¿‡åŸå› åˆ†æ
    miss_reason: Optional[str]  # é”™è¿‡çš„åŸå› 


@dataclass
class SignalPerformance:
    """ä¿¡å·è¡¨ç°ç»Ÿè®¡"""
    signal_type: str
    total_signals: int
    captured_opportunities: int  # æ•æ‰åˆ°çš„å¤§è¡Œæƒ…æ•°
    missed_opportunities: int  # é”™è¿‡çš„å¤§è¡Œæƒ…æ•°
    avg_capture_delay: float  # å¹³å‡æ•æ‰å»¶è¿Ÿ(åˆ†é’Ÿ)

    # äº¤æ˜“ç»Ÿè®¡
    total_trades: int
    winning_trades: int
    win_rate: float
    avg_pnl_pct: float
    best_trade_pnl_pct: float
    worst_trade_pnl_pct: float


@dataclass
class ReviewReport:
    """å¤ç›˜æŠ¥å‘Š"""
    date: str
    review_period: str  # '24h'

    # å¤§è¡Œæƒ…ç»Ÿè®¡
    total_opportunities: int
    captured_count: int
    missed_count: int
    capture_rate: float

    # æŒ‰æ—¶é—´å‘¨æœŸç»Ÿè®¡
    opportunities_by_timeframe: Dict[str, int]  # {'5m': 10, '15m': 5, '1h': 2}

    # é”™è¿‡çš„æœºä¼šåˆ—è¡¨
    missed_opportunities: List[BigMoveOpportunity]

    # ä¿¡å·è¡¨ç°
    signal_performances: List[SignalPerformance]

    # ä¿¡å·åˆ†æï¼ˆæ–°å¢ï¼‰
    signal_analysis: Dict[str, any]  # å„ç±»ä¿¡å·çš„è¯¦ç»†åˆ†æ

    # æœºä¼šåˆ†æï¼ˆæ–°å¢ï¼‰
    opportunity_analysis: Dict[str, any]  # äº¤æ˜“æœºä¼šåˆ†æï¼ˆä¿¡å·è¯„åˆ†ã€æ•è·æƒ…å†µã€é”™è¿‡åŸå› ï¼‰

    # ä¼˜åŒ–å»ºè®®
    optimization_suggestions: List[str]

    # å‚æ•°è°ƒæ•´å»ºè®®
    parameter_adjustments: Dict[str, any]


class DailyReviewAnalyzer:
    """æ¯æ—¥å¤ç›˜åˆ†æå™¨"""

    def __init__(self, db_config: dict):
        """
        åˆå§‹åŒ–å¤ç›˜åˆ†æå™¨

        Args:
            db_config: æ•°æ®åº“é…ç½®
        """
        self.db_config = db_config
        self.connection = None

        # å¤§è¡Œæƒ…è¯†åˆ«é˜ˆå€¼
        self.thresholds = {
            '5m': {
                'price_change_min': 0.5,  # æœ€å°ä»·æ ¼å˜åŒ– 0.5%
                'volume_ratio_min': 2.0,   # æœ€å°æˆäº¤é‡å€æ•° 2x
                'duration_candles': 3      # æŒç»­Kçº¿æ•°é‡
            },
            '15m': {
                'price_change_min': 1.0,   # 1.0%
                'volume_ratio_min': 2.0,
                'duration_candles': 2
            },
            '1h': {
                'price_change_min': 2.0,   # 2.0%
                'volume_ratio_min': 1.5,
                'duration_candles': 2
            }
        }

        logger.info("âœ… æ¯æ—¥å¤ç›˜åˆ†æå™¨å·²åˆå§‹åŒ–")

    def _get_connection(self):
        """è·å–æ•°æ®åº“è¿æ¥"""
        if self.connection is None or not self.connection.open:
            port = self.db_config.get('port', 3306)
            # ç¡®ä¿portæ˜¯æ•´æ•°ç±»å‹
            if not isinstance(port, int):
                try:
                    port = int(port)
                except (ValueError, TypeError):
                    port = 3306

            self.connection = pymysql.connect(
                host=self.db_config.get('host', 'localhost'),
                port=port,
                user=self.db_config.get('user', 'root'),
                password=self.db_config.get('password', ''),
                database=self.db_config.get('database', 'binance-data'),
                charset='utf8mb4',
                cursorclass=pymysql.cursors.DictCursor
            )
        return self.connection

    async def run_daily_review(self, symbols: List[str]) -> ReviewReport:
        """
        æ‰§è¡Œæ¯æ—¥å¤ç›˜åˆ†æ

        Args:
            symbols: è¦åˆ†æçš„äº¤æ˜“å¯¹åˆ—è¡¨

        Returns:
            å¤ç›˜æŠ¥å‘Š
        """
        logger.info(f"ğŸ” å¼€å§‹æ¯æ—¥å¤ç›˜åˆ†æ | äº¤æ˜“å¯¹æ•°é‡: {len(symbols)}")

        # åˆ†ææ—¶é—´èŒƒå›´: è¿‡å»24å°æ—¶
        end_time = datetime.now()
        start_time = end_time - timedelta(hours=24)

        # 1. è¯†åˆ«æ‰€æœ‰å¤§è¡Œæƒ…æœºä¼š
        all_opportunities = []

        for symbol in symbols:
            # åˆ†æä¸åŒæ—¶é—´å‘¨æœŸ
            for timeframe in ['5m', '15m', '1h']:
                opportunities = await self._detect_big_moves(
                    symbol, timeframe, start_time, end_time
                )
                all_opportunities.extend(opportunities)

        logger.info(f"ğŸ“Š è¯†åˆ«åˆ° {len(all_opportunities)} ä¸ªå¤§è¡Œæƒ…æœºä¼š")

        # 2. æ£€æŸ¥å“ªäº›æœºä¼šè¢«æ•æ‰åˆ°
        for opp in all_opportunities:
            await self._check_if_captured(opp)

        # 3. ç»Ÿè®¡ä¿¡å·è¡¨ç°
        signal_performances = await self._analyze_signal_performance(start_time, end_time)

        # 4. è¯¦ç»†ä¿¡å·åˆ†æï¼ˆæ–°å¢ï¼‰
        signal_analysis = await self._analyze_signals_detailed(start_time, end_time, all_opportunities)

        # 5. æœºä¼šåˆ†æï¼ˆæ–°å¢ï¼‰
        opportunity_analysis = await self._analyze_entry_opportunities(start_time, end_time, all_opportunities)

        # 6. ç”Ÿæˆä¼˜åŒ–å»ºè®®
        optimization_suggestions = self._generate_optimization_suggestions(
            all_opportunities, signal_performances
        )

        # 7. å‚æ•°è°ƒæ•´å»ºè®®
        parameter_adjustments = self._suggest_parameter_adjustments(
            all_opportunities, signal_performances
        )

        # 8. ç”ŸæˆæŠ¥å‘Š
        captured = [o for o in all_opportunities if o.captured]
        missed = [o for o in all_opportunities if not o.captured]

        timeframe_stats = {}
        for tf in ['5m', '15m', '1h']:
            timeframe_stats[tf] = len([o for o in all_opportunities if o.timeframe == tf])

        report = ReviewReport(
            date=end_time.strftime('%Y-%m-%d'),
            review_period='24h',
            total_opportunities=len(all_opportunities),
            captured_count=len(captured),
            missed_count=len(missed),
            capture_rate=len(captured) / len(all_opportunities) * 100 if all_opportunities else 0,
            opportunities_by_timeframe=timeframe_stats,
            missed_opportunities=missed[:20],  # åªä¿ç•™å‰20ä¸ª
            signal_performances=signal_performances,
            signal_analysis=signal_analysis,  # æ–°å¢
            opportunity_analysis=opportunity_analysis,  # æ–°å¢
            optimization_suggestions=optimization_suggestions,
            parameter_adjustments=parameter_adjustments
        )

        # 9. ä¿å­˜æŠ¥å‘Šåˆ°æ•°æ®åº“ï¼ˆåŒ…æ‹¬è¯¦ç»†æ•°æ®ï¼‰
        await self._save_report(report)

        # 10. ç”Ÿæˆå¯è¯»æŠ¥å‘Š
        self._print_report(report)

        logger.info(f"âœ… å¤ç›˜åˆ†æå®Œæˆ | æ•è·ç‡: {report.capture_rate:.1f}%")

        return report

    async def _detect_big_moves(
        self,
        symbol: str,
        timeframe: str,
        start_time: datetime,
        end_time: datetime
    ) -> List[BigMoveOpportunity]:
        """
        æ£€æµ‹å¤§è¡Œæƒ…æœºä¼š

        Args:
            symbol: äº¤æ˜“å¯¹
            timeframe: æ—¶é—´å‘¨æœŸ
            start_time: å¼€å§‹æ—¶é—´
            end_time: ç»“æŸæ—¶é—´

        Returns:
            å¤§è¡Œæƒ…æœºä¼šåˆ—è¡¨
        """
        conn = self._get_connection()
        cursor = conn.cursor()

        # è·å–Kçº¿æ•°æ®
        cursor.execute("""
            SELECT
                timestamp,
                open_price,
                high_price,
                low_price,
                close_price,
                volume
            FROM kline_data
            WHERE symbol = %s
            AND timeframe = %s
            AND timestamp >= %s
            AND timestamp <= %s
            AND exchange = 'binance_futures'
            ORDER BY timestamp ASC
        """, (symbol, timeframe, start_time, end_time))

        klines = cursor.fetchall()
        cursor.close()

        if len(klines) < 10:
            return []

        # è®¡ç®—å¹³å‡æˆäº¤é‡
        avg_volume = float(sum(k['volume'] for k in klines) / len(klines))

        opportunities = []
        threshold = self.thresholds[timeframe]

        # æ»‘åŠ¨çª—å£æ£€æµ‹å¤§è¡Œæƒ…
        window_size = threshold['duration_candles']

        for i in range(len(klines) - window_size + 1):
            window = klines[i:i + window_size]

            # è®¡ç®—çª—å£å†…çš„ä»·æ ¼å˜åŒ–å’Œæˆäº¤é‡
            start_price = float(window[0]['open_price'])
            prices = []
            volumes = []

            for k in window:
                prices.extend([
                    float(k['open_price']),
                    float(k['high_price']),
                    float(k['low_price']),
                    float(k['close_price'])
                ])
                volumes.append(float(k['volume']))

            max_price = max(prices)
            min_price = min(prices)
            end_price = float(window[-1]['close_price'])

            # è®¡ç®—ä»·æ ¼å˜åŒ–å’Œæˆäº¤é‡å€æ•°
            price_change_pct = abs(end_price - start_price) / start_price * 100
            avg_window_volume = sum(volumes) / len(volumes)
            volume_ratio = avg_window_volume / avg_volume if avg_volume > 0 else 0

            # åˆ¤æ–­æ˜¯å¦ä¸ºå¤§è¡Œæƒ…
            if (price_change_pct >= threshold['price_change_min'] and
                volume_ratio >= threshold['volume_ratio_min']):

                move_type = 'pump' if end_price > start_price else 'dump'

                opportunity = BigMoveOpportunity(
                    symbol=symbol,
                    start_time=window[0]['timestamp'],
                    end_time=window[-1]['timestamp'],
                    timeframe=timeframe,
                    move_type=move_type,
                    price_change_pct=price_change_pct if move_type == 'pump' else -price_change_pct,
                    volume_ratio=volume_ratio,
                    max_price=max_price,
                    min_price=min_price,
                    start_price=start_price,
                    end_price=end_price,
                    captured=False,
                    capture_delay_minutes=None,
                    signal_type=None,
                    position_pnl_pct=None,
                    miss_reason=None
                )

                opportunities.append(opportunity)

        return opportunities

    async def _check_if_captured(self, opportunity: BigMoveOpportunity):
        """
        æ£€æŸ¥å¤§è¡Œæƒ…æ˜¯å¦è¢«æ•æ‰åˆ°

        Args:
            opportunity: å¤§è¡Œæƒ…æœºä¼š
        """
        conn = self._get_connection()
        cursor = conn.cursor()

        # æŸ¥æ‰¾åœ¨æœºä¼šæ—¶é—´èŒƒå›´å†…åˆ›å»ºçš„æŒä»“
        # å…è®¸æå‰5åˆ†é’Ÿåˆ°å»¶è¿Ÿ30åˆ†é’Ÿçš„æŒä»“
        search_start = opportunity.start_time - timedelta(minutes=5)
        search_end = opportunity.end_time + timedelta(minutes=30)

        cursor.execute("""
            SELECT
                id,
                symbol,
                position_side,
                entry_signal_type,
                open_time,
                entry_price,
                mark_price,
                unrealized_pnl_pct,
                created_at
            FROM futures_positions
            WHERE symbol = %s
            AND open_time >= %s
            AND open_time <= %s
            AND status = 'closed'
            ORDER BY open_time ASC
            LIMIT 1
        """, (opportunity.symbol, search_start, search_end))

        position = cursor.fetchone()
        cursor.close()

        if position:
            # æ£€æŸ¥æ–¹å‘æ˜¯å¦åŒ¹é…
            expected_direction = 'LONG' if opportunity.move_type == 'pump' else 'SHORT'

            if position['position_side'] == expected_direction:
                # è®¡ç®—æ•æ‰å»¶è¿Ÿ
                delay_seconds = (position['open_time'] - opportunity.start_time).total_seconds()
                delay_minutes = int(delay_seconds / 60)

                opportunity.captured = True
                opportunity.capture_delay_minutes = delay_minutes
                opportunity.signal_type = position['entry_signal_type']
                opportunity.position_pnl_pct = float(position['unrealized_pnl_pct']) if position['unrealized_pnl_pct'] else None

                logger.debug(
                    f"âœ… æ•æ‰åˆ°: {opportunity.symbol} {opportunity.move_type} "
                    f"{opportunity.price_change_pct:.2f}% | å»¶è¿Ÿ{delay_minutes}åˆ†é’Ÿ"
                )
            else:
                opportunity.captured = False
                opportunity.miss_reason = f"æ–¹å‘é”™è¯¯(æœŸæœ›{expected_direction},å®é™…{position['position_side']})"
        else:
            opportunity.captured = False
            opportunity.miss_reason = "æœªäº§ç”Ÿä¿¡å·æˆ–æœªå¼€ä»“"

    async def _analyze_signal_performance(
        self,
        start_time: datetime,
        end_time: datetime
    ) -> List[SignalPerformance]:
        """
        åˆ†æä¿¡å·è¡¨ç°

        Args:
            start_time: å¼€å§‹æ—¶é—´
            end_time: ç»“æŸæ—¶é—´

        Returns:
            ä¿¡å·è¡¨ç°åˆ—è¡¨
        """
        conn = self._get_connection()
        cursor = conn.cursor()

        # è·å–æ‰€æœ‰ä¿¡å·ç±»å‹çš„ç»Ÿè®¡
        cursor.execute("""
            SELECT
                entry_signal_type,
                COUNT(*) as total_trades,
                SUM(CASE WHEN unrealized_pnl_pct > 0 THEN 1 ELSE 0 END) as winning_trades,
                AVG(unrealized_pnl_pct) as avg_pnl_pct,
                MAX(unrealized_pnl_pct) as best_trade,
                MIN(unrealized_pnl_pct) as worst_trade
            FROM futures_positions
            WHERE status = 'closed'
            AND close_time >= %s
            AND close_time <= %s
            AND entry_signal_type IS NOT NULL
            GROUP BY entry_signal_type
        """, (start_time, end_time))

        results = cursor.fetchall()
        cursor.close()

        performances = []

        for row in results:
            signal_type = row['entry_signal_type']
            total_trades = row['total_trades']
            winning_trades = row['winning_trades'] or 0

            performance = SignalPerformance(
                signal_type=signal_type,
                total_signals=total_trades,  # ç®€åŒ–ï¼Œå‡è®¾æ¯ä¸ªä¿¡å·éƒ½å¼€ä»“
                captured_opportunities=0,  # éœ€è¦ä»opportunitiesä¸­ç»Ÿè®¡
                missed_opportunities=0,
                avg_capture_delay=0.0,
                total_trades=total_trades,
                winning_trades=winning_trades,
                win_rate=winning_trades / total_trades * 100 if total_trades > 0 else 0,
                avg_pnl_pct=float(row['avg_pnl_pct']) if row['avg_pnl_pct'] else 0,
                best_trade_pnl_pct=float(row['best_trade']) if row['best_trade'] else 0,
                worst_trade_pnl_pct=float(row['worst_trade']) if row['worst_trade'] else 0
            )

            performances.append(performance)

        return performances

    def _generate_optimization_suggestions(
        self,
        opportunities: List[BigMoveOpportunity],
        performances: List[SignalPerformance]
    ) -> List[str]:
        """
        ç”Ÿæˆä¼˜åŒ–å»ºè®®

        Args:
            opportunities: å¤§è¡Œæƒ…æœºä¼šåˆ—è¡¨
            performances: ä¿¡å·è¡¨ç°åˆ—è¡¨

        Returns:
            ä¼˜åŒ–å»ºè®®åˆ—è¡¨
        """
        suggestions = []

        # 1. åˆ†æé”™è¿‡çš„æœºä¼š
        missed = [o for o in opportunities if not o.captured]

        if len(missed) > len(opportunities) * 0.3:  # é”™è¿‡è¶…è¿‡30%
            suggestions.append(
                f"âš ï¸ é”™è¿‡äº†{len(missed)}/{len(opportunities)}ä¸ªå¤§è¡Œæƒ…æœºä¼š ({len(missed)/len(opportunities)*100:.1f}%)ï¼Œ"
                "å»ºè®®é™ä½ä¿¡å·è§¦å‘é˜ˆå€¼"
            )

        # 2. æŒ‰é”™è¿‡åŸå› åˆ†ç±»
        miss_reasons = {}
        for opp in missed:
            reason = opp.miss_reason or "æœªçŸ¥åŸå› "
            miss_reasons[reason] = miss_reasons.get(reason, 0) + 1

        for reason, count in sorted(miss_reasons.items(), key=lambda x: x[1], reverse=True)[:3]:
            suggestions.append(f"ğŸ” ä¸»è¦é”™è¿‡åŸå› : {reason} ({count}æ¬¡)")

        # 3. åˆ†æå»¶è¿Ÿ
        captured_with_delay = [o for o in opportunities if o.captured and o.capture_delay_minutes is not None]

        if captured_with_delay:
            avg_delay = sum(o.capture_delay_minutes for o in captured_with_delay) / len(captured_with_delay)

            if avg_delay > 10:
                suggestions.append(
                    f"â° å¹³å‡æ•æ‰å»¶è¿Ÿ{avg_delay:.1f}åˆ†é’Ÿï¼Œå»ºè®®ä¼˜åŒ–ä¿¡å·æ£€æµ‹é€Ÿåº¦æˆ–é™ä½è§¦å‘æ¡ä»¶"
                )

        # 4. åˆ†æä¿¡å·èƒœç‡
        low_win_rate_signals = [p for p in performances if p.win_rate < 50 and p.total_trades >= 5]

        for perf in low_win_rate_signals:
            suggestions.append(
                f"ğŸ“‰ ä¿¡å· '{perf.signal_type}' èƒœç‡è¾ƒä½({perf.win_rate:.1f}%)ï¼Œ"
                f"å»ºè®®è°ƒæ•´å‚æ•°æˆ–è€ƒè™‘ç¦ç”¨"
            )

        # 5. åˆ†æä¸åŒæ—¶é—´å‘¨æœŸçš„æœºä¼šåˆ†å¸ƒ
        timeframe_counts = {}
        for opp in opportunities:
            timeframe_counts[opp.timeframe] = timeframe_counts.get(opp.timeframe, 0) + 1

        if '5m' in timeframe_counts and timeframe_counts['5m'] > sum(timeframe_counts.values()) * 0.5:
            suggestions.append(
                "ğŸ“Š 5åˆ†é’Ÿçº§åˆ«æœºä¼šè¾ƒå¤šï¼Œå»ºè®®å¢åŠ 5Mé«˜é¢‘ä¿¡å·æ£€æµ‹"
            )

        return suggestions

    def _suggest_parameter_adjustments(
        self,
        opportunities: List[BigMoveOpportunity],
        performances: List[SignalPerformance]
    ) -> Dict[str, any]:
        """
        å»ºè®®å‚æ•°è°ƒæ•´

        Args:
            opportunities: å¤§è¡Œæƒ…æœºä¼šåˆ—è¡¨
            performances: ä¿¡å·è¡¨ç°åˆ—è¡¨

        Returns:
            å‚æ•°è°ƒæ•´å»ºè®®å­—å…¸
        """
        adjustments = {}

        # 1. å¦‚æœé”™è¿‡å¤ªå¤špumpæœºä¼šï¼Œå»ºè®®é™ä½BOTTOM_REVERSAL_LONGé˜ˆå€¼
        missed_pumps = [o for o in opportunities if not o.captured and o.move_type == 'pump']

        if len(missed_pumps) >= 5:
            adjustments['BOTTOM_REVERSAL_LONG'] = {
                'current_threshold': 50,
                'suggested_threshold': 40,
                'reason': f'é”™è¿‡äº†{len(missed_pumps)}ä¸ªä¸Šæ¶¨æœºä¼š'
            }

        # 2. å¦‚æœé”™è¿‡å¤ªå¤šdumpæœºä¼šï¼Œå»ºè®®é™ä½WEAK_RALLY_SHORTé˜ˆå€¼
        missed_dumps = [o for o in opportunities if not o.captured and o.move_type == 'dump']

        if len(missed_dumps) >= 5:
            adjustments['WEAK_RALLY_SHORT'] = {
                'current_threshold': 50,
                'suggested_threshold': 40,
                'reason': f'é”™è¿‡äº†{len(missed_dumps)}ä¸ªä¸‹è·Œæœºä¼š'
            }

        # 3. å¦‚æœå¹³å‡å»¶è¿Ÿå¤ªé«˜ï¼Œå»ºè®®ç¼©çŸ­é‡‡æ ·çª—å£
        captured_with_delay = [o for o in opportunities if o.captured and o.capture_delay_minutes is not None]

        if captured_with_delay:
            avg_delay = sum(o.capture_delay_minutes for o in captured_with_delay) / len(captured_with_delay)

            if avg_delay > 10:
                adjustments['price_sampling'] = {
                    'current_window': '5m',
                    'suggested_window': '3m',
                    'reason': f'å¹³å‡å»¶è¿Ÿ{avg_delay:.1f}åˆ†é’Ÿè¿‡é«˜'
                }

        return adjustments

    async def _analyze_signals_detailed(
        self,
        start_time: datetime,
        end_time: datetime,
        opportunities: List[BigMoveOpportunity]
    ) -> Dict[str, any]:
        """
        è¯¦ç»†åˆ†æä¿¡å·è¡¨ç°

        Args:
            start_time: å¼€å§‹æ—¶é—´
            end_time: ç»“æŸæ—¶é—´
            opportunities: å¤§è¡Œæƒ…æœºä¼šåˆ—è¡¨

        Returns:
            ä¿¡å·åˆ†æå­—å…¸
        """
        conn = self._get_connection()
        cursor = conn.cursor()

        # 1. è·å–æ‰€æœ‰ä¿¡å·çš„è¯¦ç»†æ•°æ®
        cursor.execute("""
            SELECT
                entry_signal_type,
                position_side,
                entry_price,
                mark_price,
                unrealized_pnl_pct,
                open_time,
                close_time,
                symbol
            FROM futures_positions
            WHERE status = 'closed'
            AND close_time >= %s
            AND close_time <= %s
            AND entry_signal_type IS NOT NULL
            ORDER BY close_time DESC
        """, (start_time, end_time))

        trades = cursor.fetchall()
        cursor.close()

        # 2. æŒ‰ä¿¡å·ç±»å‹åˆ†ç»„ç»Ÿè®¡
        signal_stats = {}

        for trade in trades:
            signal_type = trade['entry_signal_type']

            if signal_type not in signal_stats:
                signal_stats[signal_type] = {
                    'total_trades': 0,
                    'win_trades': 0,
                    'loss_trades': 0,
                    'total_pnl': 0,
                    'best_trade': None,
                    'worst_trade': None,
                    'long_trades': 0,
                    'short_trades': 0,
                    'avg_holding_minutes': 0,
                    'captured_opportunities': 0
                }

            stats = signal_stats[signal_type]
            stats['total_trades'] += 1

            pnl = float(trade['unrealized_pnl_pct']) if trade['unrealized_pnl_pct'] else 0
            stats['total_pnl'] += pnl

            if pnl > 0:
                stats['win_trades'] += 1
            else:
                stats['loss_trades'] += 1

            if stats['best_trade'] is None or pnl > stats['best_trade']:
                stats['best_trade'] = pnl

            if stats['worst_trade'] is None or pnl < stats['worst_trade']:
                stats['worst_trade'] = pnl

            if trade['position_side'] == 'LONG':
                stats['long_trades'] += 1
            else:
                stats['short_trades'] += 1

            # è®¡ç®—æŒä»“æ—¶é•¿
            if trade['close_time'] and trade['open_time']:
                holding_time = (trade['close_time'] - trade['open_time']).total_seconds() / 60
                stats['avg_holding_minutes'] += holding_time

        # 3. è®¡ç®—å¹³å‡å€¼å’Œæ¯”ç‡
        for signal_type, stats in signal_stats.items():
            if stats['total_trades'] > 0:
                stats['win_rate'] = stats['win_trades'] / stats['total_trades'] * 100
                stats['avg_pnl'] = stats['total_pnl'] / stats['total_trades']
                stats['avg_holding_minutes'] = stats['avg_holding_minutes'] / stats['total_trades']

                # ç»Ÿè®¡æ•è·çš„å¤§è¡Œæƒ…
                stats['captured_opportunities'] = len([
                    o for o in opportunities
                    if o.captured and o.signal_type == signal_type
                ])

        # 4. ç”Ÿæˆä¿¡å·è¯„çº§
        for signal_type, stats in signal_stats.items():
            score = 0

            # èƒœç‡æƒé‡: 50%
            if stats['win_rate'] >= 60:
                score += 50
            elif stats['win_rate'] >= 50:
                score += 30
            elif stats['win_rate'] >= 40:
                score += 10

            # å¹³å‡ç›ˆäºæƒé‡: 30%
            if stats['avg_pnl'] >= 1.5:
                score += 30
            elif stats['avg_pnl'] >= 0.5:
                score += 20
            elif stats['avg_pnl'] >= 0:
                score += 10

            # æ•è·æœºä¼šæƒé‡: 20%
            if stats['captured_opportunities'] >= 5:
                score += 20
            elif stats['captured_opportunities'] >= 3:
                score += 10
            elif stats['captured_opportunities'] >= 1:
                score += 5

            # è¯„çº§
            if score >= 80:
                stats['rating'] = 'ğŸŒŸä¼˜ç§€'
            elif score >= 60:
                stats['rating'] = 'âœ…è‰¯å¥½'
            elif score >= 40:
                stats['rating'] = 'âš ï¸ä¸€èˆ¬'
            else:
                stats['rating'] = 'âŒè¾ƒå·®'

            stats['score'] = score

        return {
            'signal_stats': signal_stats,
            'total_signals': len(signal_stats),
            'summary': {
                'best_signal': max(signal_stats.items(), key=lambda x: x[1]['score'])[0] if signal_stats else None,
                'worst_signal': min(signal_stats.items(), key=lambda x: x[1]['score'])[0] if signal_stats else None
            }
        }

    async def _analyze_entry_opportunities(
        self,
        start_time: datetime,
        end_time: datetime,
        opportunities: List[BigMoveOpportunity]
    ) -> Dict[str, any]:
        """
        åˆ†æäº¤æ˜“æœºä¼šçš„æ•è·æƒ…å†µï¼ˆåŒ…æ‹¬ä¿¡å·è¯„åˆ†å¯¹æ¯”ã€æ•è·/é”™è¿‡åˆ†æï¼‰

        Args:
            start_time: å¼€å§‹æ—¶é—´
            end_time: ç»“æŸæ—¶é—´
            opportunities: å¤§è¡Œæƒ…æœºä¼šåˆ—è¡¨

        Returns:
            æœºä¼šåˆ†æå­—å…¸ï¼ˆä¿¡å·è¯„åˆ†ã€æ•è·æƒ…å†µã€é”™è¿‡åŸå› ç­‰ï¼‰
        """
        # 1. æŒ‰æ—¶é—´å‘¨æœŸç»Ÿè®¡æ•è·æƒ…å†µ
        timeframe_analysis = {}

        for tf in ['5m', '15m', '1h']:
            tf_opps = [o for o in opportunities if o.timeframe == tf]
            captured = [o for o in tf_opps if o.captured]
            missed = [o for o in tf_opps if not o.captured]

            # ç»Ÿè®¡pumpå’Œdump
            pumps = [o for o in tf_opps if o.move_type == 'pump']
            dumps = [o for o in tf_opps if o.move_type == 'dump']

            captured_pumps = [o for o in pumps if o.captured]
            captured_dumps = [o for o in dumps if o.captured]

            timeframe_analysis[tf] = {
                'total_opportunities': len(tf_opps),
                'captured': len(captured),
                'missed': len(missed),
                'capture_rate': len(captured) / len(tf_opps) * 100 if tf_opps else 0,
                'pumps': {
                    'total': len(pumps),
                    'captured': len(captured_pumps),
                    'rate': len(captured_pumps) / len(pumps) * 100 if pumps else 0
                },
                'dumps': {
                    'total': len(dumps),
                    'captured': len(captured_dumps),
                    'rate': len(captured_dumps) / len(dumps) * 100 if dumps else 0
                },
                'avg_price_change': sum(abs(o.price_change_pct) for o in tf_opps) / len(tf_opps) if tf_opps else 0,
                'avg_volume_ratio': sum(o.volume_ratio for o in tf_opps) / len(tf_opps) if tf_opps else 0
            }

        # 2. åˆ†æé”™è¿‡çš„åŸå› åˆ†å¸ƒ
        miss_reasons = {}
        for opp in opportunities:
            if not opp.captured and opp.miss_reason:
                reason = opp.miss_reason
                if reason not in miss_reasons:
                    miss_reasons[reason] = {
                        'count': 0,
                        'total_pct_change': 0,
                        'examples': []
                    }
                miss_reasons[reason]['count'] += 1
                miss_reasons[reason]['total_pct_change'] += abs(opp.price_change_pct)

                if len(miss_reasons[reason]['examples']) < 3:
                    miss_reasons[reason]['examples'].append({
                        'symbol': opp.symbol,
                        'time': opp.start_time.strftime('%H:%M'),
                        'change': abs(opp.price_change_pct),
                        'type': opp.move_type
                    })

        # è®¡ç®—å¹³å‡é”™å¤±å¹…åº¦
        for reason, data in miss_reasons.items():
            data['avg_missed_change'] = data['total_pct_change'] / data['count']

        # 3. æŒ‰äº¤æ˜“å¯¹ç»Ÿè®¡
        symbol_analysis = {}
        for opp in opportunities:
            symbol = opp.symbol
            if symbol not in symbol_analysis:
                symbol_analysis[symbol] = {
                    'total_opportunities': 0,
                    'captured': 0,
                    'missed': 0
                }

            symbol_analysis[symbol]['total_opportunities'] += 1
            if opp.captured:
                symbol_analysis[symbol]['captured'] += 1
            else:
                symbol_analysis[symbol]['missed'] += 1

        # è®¡ç®—æ•è·ç‡å¹¶æ’åº
        for symbol, stats in symbol_analysis.items():
            stats['capture_rate'] = stats['captured'] / stats['total_opportunities'] * 100 if stats['total_opportunities'] > 0 else 0

        # æ‰¾å‡ºè¡¨ç°æœ€å¥½å’Œæœ€å·®çš„äº¤æ˜“å¯¹
        sorted_symbols = sorted(symbol_analysis.items(), key=lambda x: x[1]['capture_rate'], reverse=True)

        # 4. æ—¶é—´åˆ†å¸ƒåˆ†æï¼ˆæŒ‰å°æ—¶ç»Ÿè®¡ï¼‰
        hour_analysis = {}
        for opp in opportunities:
            hour = opp.start_time.hour
            if hour not in hour_analysis:
                hour_analysis[hour] = {'total': 0, 'captured': 0}

            hour_analysis[hour]['total'] += 1
            if opp.captured:
                hour_analysis[hour]['captured'] += 1

        # è®¡ç®—æ¯å°æ—¶æ•è·ç‡
        for hour, stats in hour_analysis.items():
            stats['rate'] = stats['captured'] / stats['total'] * 100 if stats['total'] > 0 else 0

        return {
            'timeframe_analysis': timeframe_analysis,
            'miss_reasons': miss_reasons,
            'symbol_analysis': {
                'all_symbols': symbol_analysis,
                'best_symbols': sorted_symbols[:5] if sorted_symbols else [],
                'worst_symbols': sorted_symbols[-5:] if len(sorted_symbols) >= 5 else []
            },
            'hour_analysis': hour_analysis,
            'summary': {
                'total_opportunities': len(opportunities),
                'best_timeframe': max(timeframe_analysis.items(), key=lambda x: x[1]['capture_rate'])[0] if timeframe_analysis else None,
                'worst_timeframe': min(timeframe_analysis.items(), key=lambda x: x[1]['capture_rate'])[0] if timeframe_analysis else None,
                'main_miss_reason': max(miss_reasons.items(), key=lambda x: x[1]['count'])[0] if miss_reasons else None
            }
        }

    async def _save_report(self, report: ReviewReport):
        """
        ä¿å­˜å¤ç›˜æŠ¥å‘Šåˆ°æ•°æ®åº“

        Args:
            report: å¤ç›˜æŠ¥å‘Š
        """
        conn = self._get_connection()
        cursor = conn.cursor()

        try:
            # 1. åˆ›å»ºä¸»æŠ¥å‘Šè¡¨ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS daily_review_reports (
                    id INT AUTO_INCREMENT PRIMARY KEY,
                    date DATE NOT NULL,
                    report_json MEDIUMTEXT NOT NULL,
                    total_opportunities INT,
                    captured_count INT,
                    missed_count INT,
                    capture_rate FLOAT,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
                    UNIQUE KEY unique_date (date),
                    INDEX idx_date (date),
                    INDEX idx_capture_rate (capture_rate)
                )
            """)

            # 2. åˆ›å»ºæœºä¼šè¯¦æƒ…è¡¨
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS daily_review_opportunities (
                    id INT AUTO_INCREMENT PRIMARY KEY,
                    review_date DATE NOT NULL,
                    symbol VARCHAR(20) NOT NULL,
                    timeframe VARCHAR(10) NOT NULL,
                    move_type VARCHAR(10) NOT NULL,
                    start_time DATETIME NOT NULL,
                    end_time DATETIME NOT NULL,
                    price_change_pct FLOAT NOT NULL,
                    volume_ratio FLOAT NOT NULL,
                    captured BOOLEAN NOT NULL,
                    capture_delay_minutes INT,
                    signal_type VARCHAR(50),
                    position_pnl_pct FLOAT,
                    miss_reason TEXT,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    INDEX idx_review_date (review_date),
                    INDEX idx_symbol (symbol),
                    INDEX idx_captured (captured),
                    INDEX idx_timeframe (timeframe)
                )
            """)

            # 3. åˆ›å»ºä¿¡å·åˆ†æè¡¨
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS daily_review_signal_analysis (
                    id INT AUTO_INCREMENT PRIMARY KEY,
                    review_date DATE NOT NULL,
                    signal_type VARCHAR(50) NOT NULL,
                    total_trades INT NOT NULL,
                    win_trades INT NOT NULL,
                    loss_trades INT NOT NULL,
                    win_rate FLOAT NOT NULL,
                    avg_pnl FLOAT NOT NULL,
                    best_trade FLOAT,
                    worst_trade FLOAT,
                    long_trades INT NOT NULL,
                    short_trades INT NOT NULL,
                    avg_holding_minutes FLOAT,
                    captured_opportunities INT NOT NULL,
                    rating VARCHAR(20),
                    score INT,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    UNIQUE KEY unique_review_signal (review_date, signal_type),
                    INDEX idx_review_date (review_date),
                    INDEX idx_score (score)
                )
            """)

            # 4. æ’å…¥æˆ–æ›´æ–°ä¸»æŠ¥å‘Š
            cursor.execute("""
                INSERT INTO daily_review_reports
                (date, report_json, total_opportunities, captured_count, missed_count, capture_rate)
                VALUES (%s, %s, %s, %s, %s, %s)
                ON DUPLICATE KEY UPDATE
                report_json = VALUES(report_json),
                total_opportunities = VALUES(total_opportunities),
                captured_count = VALUES(captured_count),
                missed_count = VALUES(missed_count),
                capture_rate = VALUES(capture_rate)
            """, (
                report.date,
                json.dumps(asdict(report), ensure_ascii=False, default=str),
                report.total_opportunities,
                report.captured_count,
                report.missed_count,
                report.capture_rate
            ))

            # 5. åˆ é™¤å½“å¤©æ—§çš„æœºä¼šè®°å½•
            cursor.execute("""
                DELETE FROM daily_review_opportunities WHERE review_date = %s
            """, (report.date,))

            # 6. æ’å…¥æ‰€æœ‰æœºä¼šï¼ˆæ•è·çš„å’Œé”™è¿‡çš„ï¼‰
            all_opps = report.missed_opportunities if report.missed_opportunities else []

            # å¦‚æœreporté‡Œæœ‰capturedçš„æœºä¼šï¼Œä¹Ÿéœ€è¦ä¿å­˜ï¼ˆè¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå¯ä»¥åç»­ä¼˜åŒ–ï¼‰
            for opp in all_opps:
                cursor.execute("""
                    INSERT INTO daily_review_opportunities
                    (review_date, symbol, timeframe, move_type, start_time, end_time,
                     price_change_pct, volume_ratio, captured, capture_delay_minutes,
                     signal_type, position_pnl_pct, miss_reason)
                    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                """, (
                    report.date, opp.symbol, opp.timeframe, opp.move_type,
                    opp.start_time, opp.end_time, opp.price_change_pct, opp.volume_ratio,
                    opp.captured, opp.capture_delay_minutes, opp.signal_type,
                    opp.position_pnl_pct, opp.miss_reason
                ))

            # 7. åˆ é™¤å½“å¤©æ—§çš„ä¿¡å·åˆ†æ
            cursor.execute("""
                DELETE FROM daily_review_signal_analysis WHERE review_date = %s
            """, (report.date,))

            # 8. æ’å…¥ä¿¡å·åˆ†ææ•°æ®
            if report.signal_analysis and report.signal_analysis.get('signal_stats'):
                for signal_type, stats in report.signal_analysis['signal_stats'].items():
                    cursor.execute("""
                        INSERT INTO daily_review_signal_analysis
                        (review_date, signal_type, total_trades, win_trades, loss_trades,
                         win_rate, avg_pnl, best_trade, worst_trade, long_trades, short_trades,
                         avg_holding_minutes, captured_opportunities, rating, score)
                        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                    """, (
                        report.date, signal_type, stats['total_trades'],
                        stats['win_trades'], stats['loss_trades'], stats['win_rate'],
                        stats['avg_pnl'], stats['best_trade'], stats['worst_trade'],
                        stats['long_trades'], stats['short_trades'], stats['avg_holding_minutes'],
                        stats['captured_opportunities'], stats['rating'], stats['score']
                    ))

            conn.commit()
            logger.info(f"ğŸ’¾ å¤ç›˜æŠ¥å‘ŠåŠè¯¦ç»†æ•°æ®å·²ä¿å­˜åˆ°æ•°æ®åº“: {report.date}")

        except Exception as e:
            logger.error(f"ä¿å­˜å¤ç›˜æŠ¥å‘Šå¤±è´¥: {e}")
            conn.rollback()
        finally:
            cursor.close()

    def _print_report(self, report: ReviewReport):
        """
        æ‰“å°å¤ç›˜æŠ¥å‘Š

        Args:
            report: å¤ç›˜æŠ¥å‘Š
        """
        logger.info("\n" + "="*80)
        logger.info(f"ğŸ“Š æ¯æ—¥å¤ç›˜æŠ¥å‘Š - {report.date}")
        logger.info("="*80)

        logger.info(f"\nã€å¤§è¡Œæƒ…ç»Ÿè®¡ã€‘")
        logger.info(f"  æ€»æœºä¼šæ•°: {report.total_opportunities}")
        logger.info(f"  å·²æ•è·: {report.captured_count} ({report.capture_rate:.1f}%)")
        logger.info(f"  å·²é”™è¿‡: {report.missed_count}")

        logger.info(f"\nã€æŒ‰æ—¶é—´å‘¨æœŸã€‘")
        for tf, count in report.opportunities_by_timeframe.items():
            logger.info(f"  {tf}: {count}ä¸ªæœºä¼š")

        logger.info(f"\nã€é”™è¿‡çš„é‡è¦æœºä¼šã€‘(å‰5ä¸ª)")
        for i, opp in enumerate(report.missed_opportunities[:5], 1):
            logger.info(
                f"  {i}. {opp.symbol} {opp.timeframe} {opp.move_type.upper()} "
                f"{abs(opp.price_change_pct):.2f}% | {opp.volume_ratio:.1f}xé‡èƒ½"
            )
            logger.info(f"     æ—¶é—´: {opp.start_time.strftime('%H:%M')} - {opp.end_time.strftime('%H:%M')}")
            logger.info(f"     åŸå› : {opp.miss_reason}")

        logger.info(f"\nã€ä¿¡å·è¡¨ç°ã€‘")
        for perf in report.signal_performances:
            logger.info(
                f"  {perf.signal_type}: "
                f"{perf.total_trades}ç¬” | "
                f"èƒœç‡{perf.win_rate:.1f}% | "
                f"å¹³å‡{perf.avg_pnl_pct:.2f}%"
            )

        # ä¿¡å·åˆ†æï¼ˆæ–°å¢ï¼‰
        if report.signal_analysis and report.signal_analysis.get('signal_stats'):
            logger.info(f"\nã€ä¿¡å·åˆ†æã€‘")

            signal_stats = report.signal_analysis['signal_stats']

            # æŒ‰è¯„åˆ†æ’åº
            sorted_signals = sorted(signal_stats.items(), key=lambda x: x[1]['score'], reverse=True)

            for signal_type, stats in sorted_signals:
                logger.info(
                    f"  {stats['rating']} {signal_type} (è¯„åˆ†: {stats['score']})"
                )
                logger.info(
                    f"     äº¤æ˜“: {stats['total_trades']}ç¬” | "
                    f"èƒœç‡: {stats['win_rate']:.1f}% | "
                    f"å¹³å‡ç›ˆäº: {stats['avg_pnl']:.2f}%"
                )
                logger.info(
                    f"     æœ€ä½³: +{stats['best_trade']:.2f}% | "
                    f"æœ€å·®: {stats['worst_trade']:.2f}% | "
                    f"æ•è·æœºä¼š: {stats['captured_opportunities']}ä¸ª"
                )
                logger.info(
                    f"     åšå¤š: {stats['long_trades']}ç¬” | "
                    f"åšç©º: {stats['short_trades']}ç¬” | "
                    f"å¹³å‡æŒä»“: {stats['avg_holding_minutes']:.0f}åˆ†é’Ÿ"
                )

            if report.signal_analysis.get('summary'):
                summary = report.signal_analysis['summary']
                logger.info(
                    f"\n  ğŸ’¡ æœ€ä½³ä¿¡å·: {summary.get('best_signal', 'N/A')} | "
                    f"éœ€æ”¹è¿›ä¿¡å·: {summary.get('worst_signal', 'N/A')}"
                )

        # æœºä¼šåˆ†æï¼ˆæ–°å¢ï¼‰
        if report.opportunity_analysis:
            logger.info(f"\nã€æœºä¼šåˆ†æã€‘")

            tf_analysis = report.opportunity_analysis.get('timeframe_analysis', {})

            # 1. æŒ‰æ—¶é—´å‘¨æœŸå±•ç¤º
            logger.info("  æ—¶é—´å‘¨æœŸè¡¨ç°:")
            for tf in ['5m', '15m', '1h']:
                if tf in tf_analysis:
                    stats = tf_analysis[tf]
                    logger.info(
                        f"    {tf.upper()}: æœºä¼š{stats['total_opportunities']}ä¸ª | "
                        f"æ•è·{stats['captured']}ä¸ª ({stats['capture_rate']:.1f}%) | "
                        f"é”™è¿‡{stats['missed']}ä¸ª"
                    )
                    logger.info(
                        f"          ä¸Šæ¶¨: {stats['pumps']['captured']}/{stats['pumps']['total']} ({stats['pumps']['rate']:.1f}%) | "
                        f"ä¸‹è·Œ: {stats['dumps']['captured']}/{stats['dumps']['total']} ({stats['dumps']['rate']:.1f}%)"
                    )

            # 2. é”™è¿‡åŸå› åˆ†æ
            miss_reasons = report.opportunity_analysis.get('miss_reasons', {})
            if miss_reasons:
                logger.info(f"\n  ä¸»è¦é”™è¿‡åŸå› :")
                sorted_reasons = sorted(miss_reasons.items(), key=lambda x: x[1]['count'], reverse=True)[:3]

                for reason, data in sorted_reasons:
                    logger.info(
                        f"    â€¢ {reason}: {data['count']}æ¬¡ | "
                        f"å¹³å‡é”™å¤±{data['avg_missed_change']:.2f}%æ¶¨è·Œå¹…"
                    )

                    # æ˜¾ç¤ºç¤ºä¾‹
                    if data.get('examples'):
                        examples = data['examples'][:2]
                        for ex in examples:
                            logger.info(
                                f"      - {ex['symbol']} {ex['time']} {ex['type'].upper()} {ex['change']:.2f}%"
                            )

            # 3. äº¤æ˜“å¯¹è¡¨ç°
            symbol_analysis = report.opportunity_analysis.get('symbol_analysis', {})
            if symbol_analysis:
                best_symbols = symbol_analysis.get('best_symbols', [])[:3]
                worst_symbols = symbol_analysis.get('worst_symbols', [])[:3]

                if best_symbols:
                    logger.info(f"\n  æ•è·ç‡æœ€é«˜äº¤æ˜“å¯¹:")
                    for symbol, stats in best_symbols:
                        logger.info(
                            f"    âœ… {symbol}: {stats['capture_rate']:.1f}% "
                            f"({stats['captured']}/{stats['total_opportunities']})"
                        )

                if worst_symbols:
                    logger.info(f"\n  æ•è·ç‡æœ€ä½äº¤æ˜“å¯¹:")
                    for symbol, stats in worst_symbols:
                        logger.info(
                            f"    âŒ {symbol}: {stats['capture_rate']:.1f}% "
                            f"({stats['captured']}/{stats['total_opportunities']})"
                        )

            # 4. æ€»ç»“å»ºè®®
            if report.opportunity_analysis.get('summary'):
                summary = report.opportunity_analysis['summary']
                logger.info(f"\n  ğŸ“Š æ€»ç»“:")
                logger.info(f"     æœ€ä½³æ—¶é—´å‘¨æœŸ: {summary.get('best_timeframe', 'N/A')}")
                logger.info(f"     æœ€å¼±æ—¶é—´å‘¨æœŸ: {summary.get('worst_timeframe', 'N/A')}")
                logger.info(f"     ä¸»è¦é”™è¿‡åŸå› : {summary.get('main_miss_reason', 'N/A')}")

        logger.info(f"\nã€ä¼˜åŒ–å»ºè®®ã€‘")
        for suggestion in report.optimization_suggestions:
            logger.info(f"  {suggestion}")

        if report.parameter_adjustments:
            logger.info(f"\nã€å‚æ•°è°ƒæ•´å»ºè®®ã€‘")
            for param, adjustment in report.parameter_adjustments.items():
                logger.info(f"  {param}:")
                logger.info(f"    åŸå› : {adjustment['reason']}")
                if 'current_threshold' in adjustment:
                    logger.info(
                        f"    å»ºè®®: {adjustment['current_threshold']} â†’ "
                        f"{adjustment['suggested_threshold']}"
                    )

        logger.info("\n" + "="*80 + "\n")


async def main():
    """æµ‹è¯•ä¸»å‡½æ•°"""
    db_config = {
        'host': 'localhost',
        'port': 3306,
        'user': 'root',
        'password': '',
        'database': 'binance-data'
    }

    analyzer = DailyReviewAnalyzer(db_config)

    # æµ‹è¯•äº¤æ˜“å¯¹
    test_symbols = ['BTC/USDT', 'ETH/USDT', 'SOL/USDT', 'BNB/USDT']

    report = await analyzer.run_daily_review(test_symbols)

    print(f"\nå¤ç›˜å®Œæˆï¼æ•è·ç‡: {report.capture_rate:.1f}%")


if __name__ == '__main__':
    asyncio.run(main())
