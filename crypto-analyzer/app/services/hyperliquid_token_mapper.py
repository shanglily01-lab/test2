"""
Hyperliquid Token Mapper
å°† Hyperliquid çš„ @N ä»£å¸ç´¢å¼•æ˜ å°„ä¸ºå¯è¯»çš„ä»£å¸ç¬¦å·
"""
import logging
import json
import requests
from typing import Dict, Optional
from datetime import datetime, timedelta
from pathlib import Path

logger = logging.getLogger(__name__)


class HyperliquidTokenMapper:
    """Hyperliquidä»£å¸æ˜ å°„æœåŠ¡"""

    def __init__(self, cache_file: str = None):
        """
        åˆå§‹åŒ–ä»£å¸æ˜ å°„å™¨

        Args:
            cache_file: ç¼“å­˜æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä¸º data/hyperliquid_tokens.json
        """
        if cache_file is None:
            cache_file = Path(__file__).parent.parent.parent / "data" / "hyperliquid_tokens.json"

        self.cache_file = Path(cache_file)
        self.cache_file.parent.mkdir(parents=True, exist_ok=True)

        self.api_url = "https://api.hyperliquid.xyz/info"
        self.token_map: Dict[str, str] = {}  # @N -> symbol
        self.reverse_map: Dict[str, str] = {}  # symbol -> @N
        self.last_update: Optional[datetime] = None
        self.cache_duration = timedelta(hours=24)  # ç¼“å­˜24å°æ—¶

        # åŠ è½½ç¼“å­˜
        self._load_cache()

    def _load_cache(self) -> bool:
        """ä»ç¼“å­˜æ–‡ä»¶åŠ è½½ä»£å¸æ˜ å°„"""
        try:
            if not self.cache_file.exists():
                logger.info("ç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°†é¦–æ¬¡è·å–ä»£å¸æ˜ å°„")
                return False

            with open(self.cache_file, 'r', encoding='utf-8') as f:
                cache_data = json.load(f)

            self.token_map = cache_data.get('token_map', {})
            self.reverse_map = cache_data.get('reverse_map', {})
            last_update_str = cache_data.get('last_update')

            if last_update_str:
                self.last_update = datetime.fromisoformat(last_update_str)
                logger.info(f"ä»ç¼“å­˜åŠ è½½ {len(self.token_map)} ä¸ªä»£å¸æ˜ å°„ï¼Œæœ€åæ›´æ–°: {self.last_update}")
                return True

            return False

        except Exception as e:
            logger.error(f"åŠ è½½ç¼“å­˜å¤±è´¥: {e}")
            return False

    def _save_cache(self) -> bool:
        """ä¿å­˜ä»£å¸æ˜ å°„åˆ°ç¼“å­˜æ–‡ä»¶"""
        try:
            cache_data = {
                'token_map': self.token_map,
                'reverse_map': self.reverse_map,
                'last_update': self.last_update.isoformat() if self.last_update else None,
                'total_tokens': len(self.token_map)
            }

            with open(self.cache_file, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)

            logger.info(f"ä¿å­˜ {len(self.token_map)} ä¸ªä»£å¸æ˜ å°„åˆ°ç¼“å­˜")
            return True

        except Exception as e:
            logger.error(f"ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")
            return False

    def _need_update(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°æ˜ å°„"""
        if not self.token_map:
            return True

        if self.last_update is None:
            return True

        if datetime.now() - self.last_update > self.cache_duration:
            return True

        return False

    def update_token_mapping(self, force: bool = False) -> bool:
        """
        ä»Hyperliquid APIæ›´æ–°ä»£å¸æ˜ å°„

        Args:
            force: æ˜¯å¦å¼ºåˆ¶æ›´æ–°ï¼ˆå¿½ç•¥ç¼“å­˜æ—¶é—´ï¼‰

        Returns:
            bool: æ›´æ–°æˆåŠŸè¿”å›True
        """
        if not force and not self._need_update():
            logger.info("ä»£å¸æ˜ å°„ä»åœ¨æœ‰æ•ˆæœŸå†…ï¼Œè·³è¿‡æ›´æ–°")
            return True

        try:
            logger.info("æ­£åœ¨ä»Hyperliquid APIè·å–ä»£å¸æ˜ å°„...")

            # è¯·æ±‚æ‰€æœ‰ä»£å¸çš„å…ƒæ•°æ®
            response = requests.post(
                self.api_url,
                json={"type": "metaAndAssetCtxs"},
                timeout=10
            )

            if response.status_code != 200:
                logger.error(f"APIè¯·æ±‚å¤±è´¥: {response.status_code}")
                return False

            data = response.json()

            # è§£æä»£å¸ä¿¡æ¯
            if not isinstance(data, list) or len(data) < 1:
                logger.error("APIè¿”å›æ•°æ®æ ¼å¼é”™è¯¯")
                return False

            meta = data[0].get('universe', [])

            # æ„å»ºæ˜ å°„
            new_token_map = {}
            new_reverse_map = {}

            for idx, token_info in enumerate(meta):
                symbol = token_info.get('name', '')
                if symbol:
                    index_key = f"@{idx}"
                    new_token_map[index_key] = symbol
                    new_reverse_map[symbol] = index_key

            if not new_token_map:
                logger.error("æœªèƒ½è§£æåˆ°ä»»ä½•ä»£å¸ä¿¡æ¯")
                return False

            # æ›´æ–°æ˜ å°„
            self.token_map = new_token_map
            self.reverse_map = new_reverse_map
            self.last_update = datetime.now()

            logger.info(f"æˆåŠŸæ›´æ–° {len(self.token_map)} ä¸ªä»£å¸æ˜ å°„")

            # ä¿å­˜ç¼“å­˜
            self._save_cache()

            return True

        except requests.exceptions.RequestException as e:
            logger.error(f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {e}")
            return False
        except Exception as e:
            logger.error(f"æ›´æ–°ä»£å¸æ˜ å°„å¤±è´¥: {e}")
            return False

    def get_symbol(self, index: str) -> str:
        """
        è·å–ä»£å¸ç¬¦å·

        Args:
            index: ä»£å¸ç´¢å¼•ï¼Œå¦‚ "@107" æˆ– "107"

        Returns:
            str: ä»£å¸ç¬¦å·ï¼Œå¦‚æœæœªæ‰¾åˆ°åˆ™è¿”å›åŸå§‹ç´¢å¼•
        """
        # ç¡®ä¿æ˜ å°„å·²åŠ è½½
        if self._need_update():
            self.update_token_mapping()

        # æ ‡å‡†åŒ–ç´¢å¼•æ ¼å¼
        if not index.startswith('@'):
            index = f"@{index}"

        return self.token_map.get(index, index)

    def get_index(self, symbol: str) -> Optional[str]:
        """
        è·å–ä»£å¸ç´¢å¼•

        Args:
            symbol: ä»£å¸ç¬¦å·ï¼Œå¦‚ "ALT"

        Returns:
            str: ä»£å¸ç´¢å¼•ï¼Œå¦‚ "@107"ï¼Œæœªæ‰¾åˆ°è¿”å› None
        """
        # ç¡®ä¿æ˜ å°„å·²åŠ è½½
        if self._need_update():
            self.update_token_mapping()

        return self.reverse_map.get(symbol.upper())

    def format_symbol(self, symbol: str) -> str:
        """
        æ ¼å¼åŒ–ä»£å¸ç¬¦å·æ˜¾ç¤º

        Args:
            symbol: åŸå§‹ç¬¦å·ï¼ˆå¯èƒ½æ˜¯@Næˆ–æ­£å¸¸ç¬¦å·ï¼‰

        Returns:
            str: æ ¼å¼åŒ–åçš„æ˜¾ç¤ºæ–‡æœ¬
            ä¾‹å¦‚: "@107" -> "ALT (@107)"
                 "BTC" -> "BTC"
        """
        if not symbol.startswith('@'):
            return symbol

        # è·å–çœŸå®ç¬¦å·
        real_symbol = self.get_symbol(symbol)

        # å¦‚æœæ‰¾åˆ°äº†æ˜ å°„ï¼Œæ˜¾ç¤ºä¸º "ç¬¦å· (ç´¢å¼•)"
        if real_symbol != symbol:
            return f"{real_symbol} ({symbol})"

        # æœªæ‰¾åˆ°æ˜ å°„ï¼Œåªæ˜¾ç¤ºç´¢å¼•
        return symbol

    def get_all_tokens(self) -> Dict[str, str]:
        """
        è·å–æ‰€æœ‰ä»£å¸æ˜ å°„

        Returns:
            Dict[str, str]: å®Œæ•´çš„ä»£å¸æ˜ å°„å­—å…¸
        """
        if self._need_update():
            self.update_token_mapping()

        return self.token_map.copy()

    def search_tokens(self, keyword: str) -> Dict[str, str]:
        """
        æœç´¢ä»£å¸

        Args:
            keyword: æœç´¢å…³é”®è¯

        Returns:
            Dict[str, str]: åŒ¹é…çš„ä»£å¸æ˜ å°„
        """
        if self._need_update():
            self.update_token_mapping()

        keyword = keyword.upper()
        results = {}

        for index, symbol in self.token_map.items():
            if keyword in symbol.upper() or keyword in index:
                results[index] = symbol

        return results

    def get_stats(self) -> Dict:
        """
        è·å–æ˜ å°„ç»Ÿè®¡ä¿¡æ¯

        Returns:
            Dict: ç»Ÿè®¡ä¿¡æ¯
        """
        return {
            'total_tokens': len(self.token_map),
            'last_update': self.last_update.isoformat() if self.last_update else None,
            'cache_file': str(self.cache_file),
            'cache_valid': not self._need_update()
        }


# å…¨å±€å•ä¾‹
_mapper_instance: Optional[HyperliquidTokenMapper] = None


def get_token_mapper() -> HyperliquidTokenMapper:
    """è·å–å…¨å±€ä»£å¸æ˜ å°„å™¨å®ä¾‹"""
    global _mapper_instance
    if _mapper_instance is None:
        _mapper_instance = HyperliquidTokenMapper()
    return _mapper_instance


def format_hyperliquid_symbol(symbol: str) -> str:
    """
    ä¾¿æ·å‡½æ•°ï¼šæ ¼å¼åŒ–Hyperliquidä»£å¸ç¬¦å·

    Args:
        symbol: åŸå§‹ç¬¦å·

    Returns:
        str: æ ¼å¼åŒ–åçš„ç¬¦å·
    """
    mapper = get_token_mapper()
    return mapper.format_symbol(symbol)


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    logging.basicConfig(level=logging.INFO)

    mapper = HyperliquidTokenMapper()

    # æ›´æ–°æ˜ å°„
    print("ğŸ”„ æ›´æ–°ä»£å¸æ˜ å°„...")
    success = mapper.update_token_mapping(force=True)

    if success:
        print(f"\nâœ… æˆåŠŸè·å– {len(mapper.token_map)} ä¸ªä»£å¸æ˜ å°„\n")

        # æµ‹è¯•å‡ ä¸ªç´¢å¼•
        test_indices = ["@0", "@1", "@107", "@200"]
        print("ğŸ“Š æµ‹è¯•ç´¢å¼•è½¬æ¢:")
        for idx in test_indices:
            symbol = mapper.get_symbol(idx)
            formatted = mapper.format_symbol(idx)
            print(f"  {idx} -> {symbol} (æ˜¾ç¤º: {formatted})")

        # æµ‹è¯•åå‘æŸ¥è¯¢
        print("\nğŸ” æµ‹è¯•ç¬¦å·æŸ¥è¯¢:")
        test_symbols = ["BTC", "ETH", "ALT", "SOL"]
        for sym in test_symbols:
            idx = mapper.get_index(sym)
            print(f"  {sym} -> {idx}")

        # æ˜¾ç¤ºç»Ÿè®¡
        print("\nğŸ“ˆ ç»Ÿè®¡ä¿¡æ¯:")
        stats = mapper.get_stats()
        for key, value in stats.items():
            print(f"  {key}: {value}")

        # æ˜¾ç¤ºå‰20ä¸ªä»£å¸
        print("\nğŸ“‹ å‰20ä¸ªä»£å¸:")
        for i in range(min(20, len(mapper.token_map))):
            idx = f"@{i}"
            symbol = mapper.get_symbol(idx)
            print(f"  {idx}: {symbol}")
    else:
        print("âŒ æ›´æ–°å¤±è´¥")
