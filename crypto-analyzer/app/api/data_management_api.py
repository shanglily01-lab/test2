# -*- coding: utf-8 -*-
"""
æ•°æ®ç®¡ç†API
æä¾›æ•°æ®ç»Ÿè®¡ã€æŸ¥è¯¢å’Œç»´æŠ¤åŠŸèƒ½
"""

from fastapi import APIRouter, HTTPException, UploadFile, File, Form
from fastapi.responses import FileResponse
from typing import Dict, List, Optional
from datetime import datetime, timedelta
from loguru import logger
import pymysql
import yaml
from pathlib import Path
import csv
import io
import asyncio
import pandas as pd

router = APIRouter(prefix="/api/data-management", tags=["æ•°æ®ç®¡ç†"])


def get_db_config():
    """è·å–æ•°æ®åº“é…ç½®"""
    config_path = Path(__file__).parent.parent.parent / "config.yaml"
    if config_path.exists():
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        return config.get('database', {}).get('mysql', {})
    return {}


def get_db_connection():
    """è·å–æ•°æ®åº“è¿æ¥"""
    db_config = get_db_config()
    return pymysql.connect(
        host=db_config.get('host', 'localhost'),
        port=db_config.get('port', 3306),
        user=db_config.get('user', 'root'),
        password=db_config.get('password', ''),
        database=db_config.get('database', 'binance-data'),
        charset='utf8mb4',
        cursorclass=pymysql.cursors.DictCursor
    )


def _update_config_file(config_path: Path, symbols: List[str], data_type: str, timeframe: str = None) -> bool:
    """
    æ›´æ–°config.yamlæ–‡ä»¶ï¼Œæ·»åŠ æ–°çš„äº¤æ˜“å¯¹å’Œæ—¶é—´å‘¨æœŸ
    
    Args:
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        symbols: è¦æ·»åŠ çš„äº¤æ˜“å¯¹åˆ—è¡¨
        data_type: æ•°æ®ç±»å‹ ('price' æˆ– 'kline')
        timeframe: æ—¶é—´å‘¨æœŸï¼ˆä»…klineç±»å‹éœ€è¦ï¼‰
    
    Returns:
        æ˜¯å¦æˆåŠŸæ›´æ–°
    """
    try:
        # è¯»å–ç°æœ‰é…ç½®
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        updated = False
        
        # æ›´æ–°symbolsåˆ—è¡¨
        if 'symbols' not in config:
            config['symbols'] = []
        
        existing_symbols = set(config['symbols'])
        new_symbols = []
        for symbol in symbols:
            symbol = symbol.strip()
            if symbol and symbol not in existing_symbols:
                config['symbols'].append(symbol)
                new_symbols.append(symbol)
                updated = True
        
        # å¦‚æœæ˜¯Kçº¿æ•°æ®ï¼Œæ›´æ–°timeframesåˆ—è¡¨
        if data_type == 'kline' and timeframe:
            if 'collector' not in config:
                config['collector'] = {}
            if 'timeframes' not in config['collector']:
                config['collector']['timeframes'] = []
            
            existing_timeframes = set(config['collector']['timeframes'])
            if timeframe not in existing_timeframes:
                config['collector']['timeframes'].append(timeframe)
                updated = True
        
        # å¦‚æœæœ‰æ›´æ–°ï¼Œä¿å­˜é…ç½®æ–‡ä»¶
        if updated:
            with open(config_path, 'w', encoding='utf-8') as f:
                # ä½¿ç”¨æ›´å¥½çš„æ ¼å¼é€‰é¡¹ä¿æŒYAMLå¯è¯»æ€§
                yaml.dump(config, f, allow_unicode=True, default_flow_style=False, 
                         sort_keys=False, indent=2, width=120)
            logger.info(f"é…ç½®æ–‡ä»¶å·²æ›´æ–°: æ·»åŠ äº† {len(new_symbols)} ä¸ªæ–°äº¤æ˜“å¯¹" + 
                      (f"ï¼Œæ—¶é—´å‘¨æœŸ {timeframe}" if data_type == 'kline' and timeframe else ""))
        
        return updated
        
    except Exception as e:
        logger.error(f"æ›´æ–°é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def _check_status_active(latest_time, threshold_seconds):
    """
    æ£€æŸ¥æ•°æ®é‡‡é›†çŠ¶æ€æ˜¯å¦æ´»è·ƒ
    
    åˆ¤æ–­é€»è¾‘ï¼š
    - å¦‚æœæœ€æ–°æ•°æ®æ—¶é—´åœ¨é˜ˆå€¼å†…ï¼Œè¿”å› 'active'
    - å¦‚æœæœ€æ–°æ•°æ®æ—¶é—´è¶…è¿‡é˜ˆå€¼ä½†å°äºé˜ˆå€¼çš„3å€ï¼Œè¿”å› 'warning'ï¼ˆå¯èƒ½å»¶è¿Ÿï¼‰
    - å¦‚æœæœ€æ–°æ•°æ®æ—¶é—´è¶…è¿‡é˜ˆå€¼çš„3å€ï¼Œè¿”å› 'inactive'
    """
    try:
        if not latest_time:
            return 'inactive'
        
        # è½¬æ¢ä¸ºdatetimeå¯¹è±¡
        if isinstance(latest_time, str):
            # å°è¯•è§£æISOæ ¼å¼
            try:
                latest_time = datetime.fromisoformat(latest_time.replace('Z', '+00:00'))
            except:
                # å¦‚æœå¤±è´¥ï¼Œå°è¯•å…¶ä»–å¸¸è§æ ¼å¼
                try:
                    # å°è¯• MySQL datetime æ ¼å¼: YYYY-MM-DD HH:MM:SS
                    latest_time = datetime.strptime(latest_time, '%Y-%m-%d %H:%M:%S')
                except:
                    try:
                        # å°è¯•å¸¦æ¯«ç§’çš„æ ¼å¼
                        latest_time = datetime.strptime(latest_time, '%Y-%m-%d %H:%M:%S.%f')
                    except:
                        logger.error(f"æ— æ³•è§£ææ—¶é—´æ ¼å¼: {latest_time}")
                        return 'inactive'
        elif not isinstance(latest_time, datetime):
            # å¦‚æœæ˜¯å…¶ä»–ç±»å‹ï¼ˆå¦‚MySQLçš„datetimeå¯¹è±¡ï¼‰ï¼Œå°è¯•è½¬æ¢
            if hasattr(latest_time, 'isoformat'):
                try:
                    latest_time = datetime.fromisoformat(latest_time.isoformat())
                except:
                    latest_time = datetime.strptime(str(latest_time), '%Y-%m-%d %H:%M:%S')
            else:
                try:
                    latest_time = datetime.strptime(str(latest_time), '%Y-%m-%d %H:%M:%S')
                except:
                    logger.error(f"æ— æ³•è½¬æ¢æ—¶é—´ç±»å‹: {type(latest_time)}, value: {latest_time}")
                    return 'inactive'
        
        # å¤„ç†æ—¶åŒºï¼šç»Ÿä¸€è½¬æ¢ä¸ºæ— æ—¶åŒºçš„æœ¬åœ°æ—¶é—´è¿›è¡Œæ¯”è¾ƒ
        if latest_time.tzinfo is not None:
            # å¦‚æœæœ‰æ—¶åŒºä¿¡æ¯ï¼Œè½¬æ¢ä¸ºæœ¬åœ°æ—¶é—´
            latest_time = latest_time.replace(tzinfo=None)
        
        # è®¡ç®—æ—¶é—´å·®ï¼ˆç§’ï¼‰
        now = datetime.now()
        time_diff = (now - latest_time).total_seconds()
        
        # åˆ¤æ–­çŠ¶æ€
        if time_diff < 0:
            # æœªæ¥æ—¶é—´ï¼Œå¯èƒ½æ˜¯æ—¶åŒºé—®é¢˜ï¼Œè§†ä¸ºæ´»è·ƒ
            return 'active'
        elif time_diff <= threshold_seconds:
            return 'active'
        elif time_diff <= threshold_seconds * 3:
            return 'warning'  # å»¶è¿Ÿä½†å¯èƒ½è¿˜åœ¨è¿è¡Œ
        else:
            return 'inactive'
            
    except Exception as e:
        logger.error(f"æ£€æŸ¥çŠ¶æ€å¤±è´¥: {e}, latest_time={latest_time}, type={type(latest_time)}")
        import traceback
        traceback.print_exc()
        return 'inactive'


@router.get("/statistics")
async def get_data_statistics():
    """
    è·å–æ‰€æœ‰æ•°æ®è¡¨çš„ç»Ÿè®¡ä¿¡æ¯
    åŒ…æ‹¬è®°å½•æ•°ã€æœ€æ–°æ•°æ®æ—¶é—´ã€æ•°æ®èŒƒå›´ç­‰
    """
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        # å®šä¹‰æ‰€æœ‰æ•°æ®è¡¨åŠå…¶æè¿°
        tables = [
            {'name': 'price_data', 'label': 'å®æ—¶ä»·æ ¼æ•°æ®', 'description': 'å­˜å‚¨å„äº¤æ˜“æ‰€çš„å®æ—¶ä»·æ ¼æ•°æ®'},
            {'name': 'kline_data', 'label': 'Kçº¿æ•°æ®', 'description': 'å­˜å‚¨ä¸åŒæ—¶é—´å‘¨æœŸçš„Kçº¿æ•°æ®'},
            {'name': 'news_data', 'label': 'æ–°é—»æ•°æ®', 'description': 'å­˜å‚¨åŠ å¯†è´§å¸ç›¸å…³æ–°é—»'},
            {'name': 'funding_rate_data', 'label': 'èµ„é‡‘è´¹ç‡æ•°æ®', 'description': 'å­˜å‚¨åˆçº¦èµ„é‡‘è´¹ç‡æ•°æ®'},
            {'name': 'futures_open_interest', 'label': 'æŒä»“é‡æ•°æ®', 'description': 'å­˜å‚¨åˆçº¦æŒä»“é‡æ•°æ®'},
            {'name': 'futures_long_short_ratio', 'label': 'å¤šç©ºæ¯”æ•°æ®', 'description': 'å­˜å‚¨åˆçº¦å¤šç©ºæ¯”æ•°æ®'},
            {'name': 'smart_money_transactions', 'label': 'èªæ˜é’±äº¤æ˜“', 'description': 'å­˜å‚¨èªæ˜é’±åŒ…çš„äº¤æ˜“è®°å½•'},
            {'name': 'smart_money_signals', 'label': 'èªæ˜é’±ä¿¡å·', 'description': 'å­˜å‚¨èªæ˜é’±äº¤æ˜“ä¿¡å·'},
            {'name': 'ema_signals', 'label': 'EMAä¿¡å·', 'description': 'å­˜å‚¨EMAæŠ€æœ¯æŒ‡æ ‡ä¿¡å·'},
            {'name': 'investment_recommendations', 'label': 'æŠ•èµ„å»ºè®®', 'description': 'å­˜å‚¨AIç”Ÿæˆçš„æŠ•èµ„å»ºè®®'},
            {'name': 'futures_positions', 'label': 'åˆçº¦æŒä»“', 'description': 'å­˜å‚¨åˆçº¦äº¤æ˜“æŒä»“è®°å½•'},
            {'name': 'futures_orders', 'label': 'åˆçº¦è®¢å•', 'description': 'å­˜å‚¨åˆçº¦äº¤æ˜“è®¢å•è®°å½•'},
            {'name': 'futures_trades', 'label': 'åˆçº¦äº¤æ˜“', 'description': 'å­˜å‚¨åˆçº¦äº¤æ˜“è®°å½•'},
            {'name': 'paper_trading_accounts', 'label': 'æ¨¡æ‹Ÿè´¦æˆ·', 'description': 'å­˜å‚¨æ¨¡æ‹Ÿäº¤æ˜“è´¦æˆ·ä¿¡æ¯'},
            {'name': 'paper_trading_orders', 'label': 'æ¨¡æ‹Ÿè®¢å•', 'description': 'å­˜å‚¨æ¨¡æ‹Ÿäº¤æ˜“è®¢å•è®°å½•'},
            {'name': 'paper_trading_positions', 'label': 'æ¨¡æ‹ŸæŒä»“', 'description': 'å­˜å‚¨æ¨¡æ‹Ÿäº¤æ˜“æŒä»“è®°å½•'},
            {'name': 'crypto_etf_flows', 'label': 'ETFæ•°æ®', 'description': 'å­˜å‚¨åŠ å¯†è´§å¸ETFèµ„é‡‘æµå‘æ•°æ®'},
            {'name': 'corporate_treasury_financing', 'label': 'ä¼ä¸šèèµ„', 'description': 'å­˜å‚¨ä¼ä¸šé‡‘åº“èèµ„æ•°æ®'},
        ]
        
        statistics = []
        
        for table in tables:
            try:
                # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
                cursor.execute(f"SHOW TABLES LIKE '{table['name']}'")
                if not cursor.fetchone():
                    statistics.append({
                        **table,
                        'exists': False,
                        'count': 0,
                        'latest_time': None,
                        'oldest_time': None,
                        'size_mb': 0
                    })
                    continue
                
                # è·å–è®°å½•æ•°
                cursor.execute(f"SELECT COUNT(*) as count FROM {table['name']}")
                count_result = cursor.fetchone()
                count = count_result['count'] if count_result else 0
                
                # è·å–æœ€æ–°å’Œæœ€æ—©çš„æ—¶é—´æˆ³
                latest_time = None
                oldest_time = None
                
                # å°è¯•ä¸åŒçš„æ—¶é—´å­—æ®µ
                time_fields = ['timestamp', 'created_at', 'updated_at', 'open_time', 'trade_time', 'date']
                for field in time_fields:
                    try:
                        cursor.execute(f"SELECT MAX({field}) as max_time, MIN({field}) as min_time FROM {table['name']}")
                        time_result = cursor.fetchone()
                        if time_result and time_result.get('max_time'):
                            latest_time = time_result['max_time'].isoformat() if hasattr(time_result['max_time'], 'isoformat') else str(time_result['max_time'])
                            oldest_time = time_result['min_time'].isoformat() if hasattr(time_result['min_time'], 'isoformat') else str(time_result['min_time'])
                            break
                    except:
                        continue
                
                # è·å–è¡¨å¤§å°ï¼ˆMBï¼‰
                cursor.execute(f"""
                    SELECT 
                        ROUND(((data_length + index_length) / 1024 / 1024), 2) AS size_mb
                    FROM information_schema.TABLES 
                    WHERE table_schema = DATABASE()
                    AND table_name = '{table['name']}'
                """)
                size_result = cursor.fetchone()
                size_mb = size_result['size_mb'] if size_result and size_result.get('size_mb') else 0
                
                statistics.append({
                    **table,
                    'exists': True,
                    'count': count,
                    'latest_time': latest_time,
                    'oldest_time': oldest_time,
                    'size_mb': float(size_mb) if size_mb else 0
                })
                
            except Exception as e:
                logger.error(f"è·å–è¡¨ {table['name']} ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
                statistics.append({
                    **table,
                    'exists': False,
                    'count': 0,
                    'latest_time': None,
                    'oldest_time': None,
                    'size_mb': 0,
                    'error': str(e)
                })
        
        cursor.close()
        conn.close()
        
        # è®¡ç®—æ€»è®¡
        total_count = sum(s['count'] for s in statistics)
        total_size = sum(s['size_mb'] for s in statistics)
        
        return {
            'success': True,
            'data': {
                'tables': statistics,
                'summary': {
                    'total_tables': len(statistics),
                    'total_records': total_count,
                    'total_size_mb': round(total_size, 2)
                }
            }
        }
        
    except Exception as e:
        logger.error(f"è·å–æ•°æ®ç»Ÿè®¡å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–æ•°æ®ç»Ÿè®¡å¤±è´¥: {str(e)}")


@router.get("/table/{table_name}/sample")
async def get_table_sample(table_name: str, limit: int = 10):
    """
    è·å–æŒ‡å®šè¡¨çš„æ ·æœ¬æ•°æ®
    
    Args:
        table_name: è¡¨å
        limit: è¿”å›è®°å½•æ•°é™åˆ¶
    """
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        # å®‰å…¨æ£€æŸ¥ï¼šåªå…è®¸æŸ¥è¯¢ç™½åå•ä¸­çš„è¡¨
        allowed_tables = [
            'price_data', 'kline_data', 'news_data', 'funding_rate_data',
            'futures_open_interest', 'futures_long_short_ratio',
            'smart_money_transactions', 'smart_money_signals', 'ema_signals',
            'investment_recommendations', 'futures_positions', 'futures_orders',
            'futures_trades', 'paper_trading_accounts', 'paper_trading_orders',
            'paper_trading_positions', 'crypto_etf_flows', 'corporate_treasury_financing'
        ]
        
        if table_name not in allowed_tables:
            raise HTTPException(status_code=400, detail=f"ä¸å…è®¸æŸ¥è¯¢è¡¨: {table_name}")
        
        # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
        cursor.execute(f"SHOW TABLES LIKE '{table_name}'")
        if not cursor.fetchone():
            raise HTTPException(status_code=404, detail=f"è¡¨ {table_name} ä¸å­˜åœ¨")
        
        # è·å–æ ·æœ¬æ•°æ®ï¼ˆå°è¯•ä¸åŒçš„æ’åºå­—æ®µï¼‰
        order_fields = ['id', 'timestamp', 'created_at', 'updated_at', 'open_time', 'trade_time', 'date']
        rows = []
        
        for field in order_fields:
            try:
                cursor.execute(f"SELECT * FROM {table_name} ORDER BY {field} DESC LIMIT %s", (limit,))
                rows = cursor.fetchall()
                if rows:
                    break
            except:
                continue
        
        # å¦‚æœæ‰€æœ‰æ’åºå­—æ®µéƒ½å¤±è´¥ï¼Œç›´æ¥æŸ¥è¯¢
        if not rows:
            cursor.execute(f"SELECT * FROM {table_name} LIMIT %s", (limit,))
            rows = cursor.fetchall()
        
        # è½¬æ¢æ•°æ®æ ¼å¼
        sample_data = []
        for row in rows:
            row_dict = {}
            for key, value in row.items():
                if isinstance(value, datetime):
                    row_dict[key] = value.isoformat()
                elif isinstance(value, (int, float)):
                    row_dict[key] = value
                else:
                    row_dict[key] = str(value) if value is not None else None
            sample_data.append(row_dict)
        
        cursor.close()
        conn.close()
        
        return {
            'success': True,
            'table_name': table_name,
            'count': len(sample_data),
            'data': sample_data
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–è¡¨ {table_name} æ ·æœ¬æ•°æ®å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–æ ·æœ¬æ•°æ®å¤±è´¥: {str(e)}")


@router.delete("/table/{table_name}/cleanup")
async def cleanup_old_data(
    table_name: str,
    days: int = 30,
    confirm: bool = False
):
    """
    æ¸…ç†æŒ‡å®šè¡¨çš„æ—§æ•°æ®
    
    Args:
        table_name: è¡¨å
        days: ä¿ç•™æœ€è¿‘Nå¤©çš„æ•°æ®
        confirm: ç¡®è®¤åˆ é™¤ï¼ˆå¿…é¡»ä¸ºTrueæ‰èƒ½æ‰§è¡Œï¼‰
    """
    if not confirm:
        raise HTTPException(status_code=400, detail="å¿…é¡»è®¾ç½® confirm=true æ‰èƒ½æ‰§è¡Œåˆ é™¤æ“ä½œ")
    
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        # å®‰å…¨æ£€æŸ¥ï¼šåªå…è®¸æ¸…ç†ç™½åå•ä¸­çš„è¡¨
        allowed_tables = [
            'price_data', 'kline_data', 'news_data', 'funding_rate_data',
            'futures_open_interest', 'futures_long_short_ratio',
            'smart_money_transactions', 'ema_signals'
        ]
        
        if table_name not in allowed_tables:
            raise HTTPException(status_code=400, detail=f"ä¸å…è®¸æ¸…ç†è¡¨: {table_name}")
        
        # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
        cursor.execute(f"SHOW TABLES LIKE '{table_name}'")
        if not cursor.fetchone():
            raise HTTPException(status_code=404, detail=f"è¡¨ {table_name} ä¸å­˜åœ¨")
        
        # è·å–åˆ é™¤å‰çš„è®°å½•æ•°
        cursor.execute(f"SELECT COUNT(*) as count FROM {table_name}")
        before_count = cursor.fetchone()['count']
        
        # å°è¯•ä¸åŒçš„æ—¶é—´å­—æ®µ
        time_fields = ['timestamp', 'created_at', 'updated_at', 'open_time', 'trade_time', 'date']
        deleted_count = 0
        
        for field in time_fields:
            try:
                cursor.execute(f"SELECT COUNT(*) as count FROM {table_name} WHERE {field} < DATE_SUB(NOW(), INTERVAL %s DAY)", (days,))
                old_count = cursor.fetchone()['count']
                
                if old_count > 0:
                    cursor.execute(f"DELETE FROM {table_name} WHERE {field} < DATE_SUB(NOW(), INTERVAL %s DAY)", (days,))
                    deleted_count = cursor.rowcount
                    conn.commit()
                    break
            except:
                continue
        
        # è·å–åˆ é™¤åçš„è®°å½•æ•°
        cursor.execute(f"SELECT COUNT(*) as count FROM {table_name}")
        after_count = cursor.fetchone()['count']
        
        cursor.close()
        conn.close()
        
        return {
            'success': True,
            'table_name': table_name,
            'before_count': before_count,
            'deleted_count': deleted_count,
            'after_count': after_count,
            'message': f'æˆåŠŸæ¸…ç† {deleted_count} æ¡è¶…è¿‡ {days} å¤©çš„æ—§æ•°æ®'
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æ¸…ç†è¡¨ {table_name} æ•°æ®å¤±è´¥: {e}")
        if conn:
            conn.rollback()
        raise HTTPException(status_code=500, detail=f"æ¸…ç†æ•°æ®å¤±è´¥: {str(e)}")


@router.get("/collection-status")
async def get_collection_status():
    """
    è·å–å„ç±»æ•°æ®çš„é‡‡é›†æƒ…å†µ
    åŒ…æ‹¬å®æ—¶æ•°æ®ã€åˆçº¦æ•°æ®ã€æ–°é—»æ•°æ®ç­‰çš„æœ€æ–°é‡‡é›†æ—¶é—´
    """
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        collection_status = []
        
        # 1. å®æ—¶ä»·æ ¼æ•°æ®é‡‡é›†æƒ…å†µ
        try:
            cursor.execute("""
                SELECT 
                    COUNT(*) as count,
                    MAX(timestamp) as latest_time,
                    MIN(timestamp) as oldest_time,
                    COUNT(DISTINCT symbol) as symbol_count,
                    COUNT(DISTINCT exchange) as exchange_count
                FROM price_data
            """)
            price_result = cursor.fetchone()
            collection_status.append({
                'type': 'å®æ—¶ä»·æ ¼æ•°æ®',
                'category': 'market_data',
                'icon': 'bi-graph-up',
                'description': 'å„äº¤æ˜“æ‰€çš„å®æ—¶ä»·æ ¼æ•°æ®',
                'count': price_result['count'] if price_result else 0,
                'latest_time': price_result['latest_time'].isoformat() if price_result and price_result['latest_time'] else None,
                'oldest_time': price_result['oldest_time'].isoformat() if price_result and price_result['oldest_time'] else None,
                'symbol_count': price_result['symbol_count'] if price_result else 0,
                'exchange_count': price_result['exchange_count'] if price_result else 0,
                'status': _check_status_active(price_result['latest_time'], 600) if price_result and price_result['latest_time'] else 'inactive'  # ä»·æ ¼æ•°æ®æ¯1åˆ†é’Ÿé‡‡é›†ï¼Œé˜ˆå€¼è®¾ä¸º10åˆ†é’Ÿ
            })
        except Exception as e:
            logger.error(f"è·å–å®æ—¶ä»·æ ¼æ•°æ®é‡‡é›†æƒ…å†µå¤±è´¥: {e}")
            collection_status.append({
                'type': 'å®æ—¶ä»·æ ¼æ•°æ®',
                'category': 'market_data',
                'icon': 'bi-graph-up',
                'description': 'å„äº¤æ˜“æ‰€çš„å®æ—¶ä»·æ ¼æ•°æ®',
                'status': 'error',
                'error': str(e)
            })
        
        # 2. Kçº¿æ•°æ®é‡‡é›†æƒ…å†µ
        try:
            # å°è¯•ä½¿ç”¨timestampå­—æ®µï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨created_at
            try:
                cursor.execute("""
                    SELECT 
                        COUNT(*) as count,
                        MAX(timestamp) as latest_time,
                        MIN(timestamp) as oldest_time,
                        COUNT(DISTINCT symbol) as symbol_count,
                        COUNT(DISTINCT timeframe) as timeframe_count
                    FROM kline_data
                """)
            except:
                # å¦‚æœtimestampå­—æ®µä¸å­˜åœ¨æˆ–å‡ºé”™ï¼Œä½¿ç”¨created_at
                cursor.execute("""
                    SELECT 
                        COUNT(*) as count,
                        MAX(created_at) as latest_time,
                        MIN(created_at) as oldest_time,
                        COUNT(DISTINCT symbol) as symbol_count,
                        COUNT(DISTINCT timeframe) as timeframe_count
                    FROM kline_data
                """)
            kline_result = cursor.fetchone()
            
            # å¯¹äºKçº¿æ•°æ®ï¼Œä½¿ç”¨created_atå­—æ®µæ¥åˆ¤æ–­çŠ¶æ€ï¼ˆæœ¬åœ°æ—¶é—´ï¼Œæ›´å‡†ç¡®ï¼‰
            # å› ä¸ºtimestampå­—æ®µå¯èƒ½æ˜¯UTCæ—¶é—´ï¼Œä¼šå¯¼è‡´æ—¶åŒºåˆ¤æ–­é”™è¯¯
            # ä¸åŒæ—¶é—´å‘¨æœŸçš„é‡‡é›†é¢‘ç‡ä¸åŒï¼Œä½¿ç”¨æœ€ä¸¥æ ¼çš„æ—¶é—´å‘¨æœŸï¼ˆ1mï¼‰æ¥åˆ¤æ–­
            # ä½†ä¹Ÿè¦è€ƒè™‘å…¶ä»–æ—¶é—´å‘¨æœŸï¼ˆ5m, 15m, 1h, 1dï¼‰çš„é‡‡é›†é¢‘ç‡
            status = 'inactive'
            
            # æ£€æŸ¥1åˆ†é’ŸKçº¿çš„æœ€æ–°created_atæ—¶é—´ï¼ˆæœ€ä¸¥æ ¼ï¼Œé˜ˆå€¼10åˆ†é’Ÿï¼‰
            cursor.execute("""
                SELECT MAX(created_at) as latest_created
                FROM kline_data
                WHERE timeframe = '1m' AND created_at IS NOT NULL
            """)
            kline_1m_result = cursor.fetchone()
            if kline_1m_result and kline_1m_result['latest_created']:
                status_1m = _check_status_active(kline_1m_result['latest_created'], 600)  # 10åˆ†é’Ÿé˜ˆå€¼
                if status_1m == 'active':
                    status = 'active'
                elif status_1m == 'warning' and status != 'active':
                    status = 'warning'
            
            # å¦‚æœ1åˆ†é’Ÿæ•°æ®ä¸æ´»è·ƒï¼Œæ£€æŸ¥å…¶ä»–æ—¶é—´å‘¨æœŸ
            if status == 'inactive':
                # æ£€æŸ¥5åˆ†é’ŸKçº¿ï¼ˆé˜ˆå€¼30åˆ†é’Ÿï¼‰
                cursor.execute("""
                    SELECT MAX(created_at) as latest_created
                    FROM kline_data
                    WHERE timeframe = '5m' AND created_at IS NOT NULL
                """)
                kline_5m_result = cursor.fetchone()
                if kline_5m_result and kline_5m_result['latest_created']:
                    status_5m = _check_status_active(kline_5m_result['latest_created'], 1800)  # 30åˆ†é’Ÿé˜ˆå€¼
                    if status_5m == 'active':
                        status = 'active'
                    elif status_5m == 'warning' and status != 'active':
                        status = 'warning'
                
                # æ£€æŸ¥15åˆ†é’ŸKçº¿ï¼ˆé˜ˆå€¼1å°æ—¶ï¼‰
                cursor.execute("""
                    SELECT MAX(created_at) as latest_created
                    FROM kline_data
                    WHERE timeframe = '15m' AND created_at IS NOT NULL
                """)
                kline_15m_result = cursor.fetchone()
                if kline_15m_result and kline_15m_result['latest_created']:
                    status_15m = _check_status_active(kline_15m_result['latest_created'], 3600)  # 1å°æ—¶é˜ˆå€¼
                    if status_15m == 'active':
                        status = 'active'
                    elif status_15m == 'warning' and status != 'active':
                        status = 'warning'
                
                # æ£€æŸ¥1å°æ—¶Kçº¿ï¼ˆé˜ˆå€¼3å°æ—¶ï¼‰
                cursor.execute("""
                    SELECT MAX(created_at) as latest_created
                    FROM kline_data
                    WHERE timeframe = '1h' AND created_at IS NOT NULL
                """)
                kline_1h_result = cursor.fetchone()
                if kline_1h_result and kline_1h_result['latest_created']:
                    status_1h = _check_status_active(kline_1h_result['latest_created'], 10800)  # 3å°æ—¶é˜ˆå€¼
                    if status_1h == 'active':
                        status = 'active'
                    elif status_1h == 'warning' and status != 'active':
                        status = 'warning'
                
                # æ£€æŸ¥1å¤©Kçº¿ï¼ˆé˜ˆå€¼2å¤©ï¼‰
                cursor.execute("""
                    SELECT MAX(created_at) as latest_created
                    FROM kline_data
                    WHERE timeframe = '1d' AND created_at IS NOT NULL
                """)
                kline_1d_result = cursor.fetchone()
                if kline_1d_result and kline_1d_result['latest_created']:
                    status_1d = _check_status_active(kline_1d_result['latest_created'], 172800)  # 2å¤©é˜ˆå€¼
                    if status_1d == 'active':
                        status = 'active'
                    elif status_1d == 'warning' and status != 'active':
                        status = 'warning'
            
            # å¦‚æœcreated_atå­—æ®µä¸ºç©ºï¼Œå›é€€åˆ°ä½¿ç”¨timestampå­—æ®µï¼ˆè€ƒè™‘æ—¶åŒºé—®é¢˜ï¼‰
            if status == 'inactive':
                latest_time = kline_result['latest_time'] if kline_result else None
                if latest_time:
                    # timestampå¯èƒ½æ˜¯UTCæ—¶é—´ï¼Œéœ€è¦åŠ ä¸Š8å°æ—¶ï¼ˆUTC+8ï¼‰æ¥åˆ¤æ–­
                    # æˆ–è€…ç›´æ¥ä½¿ç”¨timestampï¼Œä½†é˜ˆå€¼æ”¾å®½
                    status = _check_status_active(latest_time, 1800)  # æ”¾å®½åˆ°30åˆ†é’Ÿé˜ˆå€¼
            
            # è·å–æœ€æ–°çš„created_atæ—¶é—´ç”¨äºæ˜¾ç¤ºï¼ˆæ›´å‡†ç¡®ï¼‰
            cursor.execute("""
                SELECT MAX(created_at) as latest_created
                FROM kline_data
                WHERE created_at IS NOT NULL
            """)
            latest_created_result = cursor.fetchone()
            display_time = latest_created_result['latest_created'] if latest_created_result and latest_created_result['latest_created'] else latest_time
            
            collection_status.append({
                'type': 'Kçº¿æ•°æ®',
                'category': 'market_data',
                'icon': 'bi-bar-chart',
                'description': 'ä¸åŒæ—¶é—´å‘¨æœŸçš„Kçº¿æ•°æ®',
                'count': kline_result['count'] if kline_result else 0,
                'latest_time': display_time.isoformat() if display_time else None,
                'oldest_time': kline_result['oldest_time'].isoformat() if kline_result and kline_result['oldest_time'] else None,
                'symbol_count': kline_result['symbol_count'] if kline_result else 0,
                'timeframe_count': kline_result['timeframe_count'] if kline_result else 0,
                'status': status
            })
        except Exception as e:
            logger.error(f"è·å–Kçº¿æ•°æ®é‡‡é›†æƒ…å†µå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            collection_status.append({
                'type': 'Kçº¿æ•°æ®',
                'category': 'market_data',
                'icon': 'bi-bar-chart',
                'description': 'ä¸åŒæ—¶é—´å‘¨æœŸçš„Kçº¿æ•°æ®',
                'status': 'error',
                'error': str(e)
            })
        
        # 3. åˆçº¦æ•°æ®é‡‡é›†æƒ…å†µ
        try:
            cursor.execute("""
                SELECT 
                    COUNT(*) as count,
                    MAX(timestamp) as latest_time,
                    MIN(timestamp) as oldest_time
                FROM futures_open_interest
            """)
            futures_result = cursor.fetchone()
            collection_status.append({
                'type': 'åˆçº¦æ•°æ®',
                'category': 'futures_data',
                'icon': 'bi-graph-up-arrow',
                'description': 'åˆçº¦æŒä»“é‡ã€èµ„é‡‘è´¹ç‡ã€å¤šç©ºæ¯”ç­‰æ•°æ®',
                'count': futures_result['count'] if futures_result else 0,
                'latest_time': futures_result['latest_time'].isoformat() if futures_result and futures_result['latest_time'] else None,
                'oldest_time': futures_result['oldest_time'].isoformat() if futures_result and futures_result['oldest_time'] else None,
                'status': _check_status_active(futures_result['latest_time'], 1200) if futures_result and futures_result['latest_time'] else 'inactive'  # åˆçº¦æ•°æ®æ¯1åˆ†é’Ÿé‡‡é›†ï¼Œé˜ˆå€¼è®¾ä¸º20åˆ†é’Ÿ
            })
        except Exception as e:
            logger.error(f"è·å–åˆçº¦æ•°æ®é‡‡é›†æƒ…å†µå¤±è´¥: {e}")
            collection_status.append({
                'type': 'åˆçº¦æ•°æ®',
                'category': 'futures_data',
                'icon': 'bi-graph-up-arrow',
                'description': 'åˆçº¦æŒä»“é‡ã€èµ„é‡‘è´¹ç‡ã€å¤šç©ºæ¯”ç­‰æ•°æ®',
                'status': 'error',
                'error': str(e)
            })
        
        # 4. æ–°é—»æ•°æ®é‡‡é›†æƒ…å†µ
        try:
            # æ–°é—»æ•°æ®è¡¨ä½¿ç”¨published_datetimeå­—æ®µï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨created_at
            try:
                cursor.execute("""
                    SELECT 
                        COUNT(*) as count,
                        MAX(published_datetime) as latest_time,
                        MIN(published_datetime) as oldest_time,
                        COUNT(DISTINCT source) as source_count
                    FROM news_data
                """)
            except:
                # å¦‚æœpublished_datetimeå­—æ®µä¸å­˜åœ¨æˆ–å‡ºé”™ï¼Œä½¿ç”¨created_at
                cursor.execute("""
                    SELECT 
                        COUNT(*) as count,
                        MAX(created_at) as latest_time,
                        MIN(created_at) as oldest_time,
                        COUNT(DISTINCT source) as source_count
                    FROM news_data
                """)
            news_result = cursor.fetchone()
            collection_status.append({
                'type': 'æ–°é—»æ•°æ®',
                'category': 'news_data',
                'icon': 'bi-newspaper',
                'description': 'åŠ å¯†è´§å¸ç›¸å…³æ–°é—»',
                'count': news_result['count'] if news_result else 0,
                'latest_time': news_result['latest_time'].isoformat() if news_result and news_result['latest_time'] else None,
                'oldest_time': news_result['oldest_time'].isoformat() if news_result and news_result['oldest_time'] else None,
                'source_count': news_result['source_count'] if news_result else 0,
                'status': _check_status_active(news_result['latest_time'], 3600) if news_result and news_result['latest_time'] else 'inactive'
            })
        except Exception as e:
            logger.error(f"è·å–æ–°é—»æ•°æ®é‡‡é›†æƒ…å†µå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            collection_status.append({
                'type': 'æ–°é—»æ•°æ®',
                'category': 'news_data',
                'icon': 'bi-newspaper',
                'description': 'åŠ å¯†è´§å¸ç›¸å…³æ–°é—»',
                'status': 'error',
                'error': str(e)
            })
        
        # 5. ETFæ•°æ®æƒ…å†µ
        try:
            cursor.execute("""
                SELECT 
                    COUNT(*) as count,
                    MAX(trade_date) as latest_time,
                    MIN(trade_date) as oldest_time,
                    COUNT(DISTINCT ticker) as etf_count
                FROM crypto_etf_flows
            """)
            etf_result = cursor.fetchone()
            collection_status.append({
                'type': 'ETFæ•°æ®',
                'category': 'etf_data',
                'icon': 'bi-pie-chart',
                'description': 'åŠ å¯†è´§å¸ETFèµ„é‡‘æµå‘æ•°æ®',
                'count': etf_result['count'] if etf_result else 0,
                'latest_time': etf_result['latest_time'].isoformat() if etf_result and etf_result['latest_time'] else None,
                'oldest_time': etf_result['oldest_time'].isoformat() if etf_result and etf_result['oldest_time'] else None,
                'etf_count': etf_result['etf_count'] if etf_result else 0,
                'status': 'active' if etf_result and etf_result['latest_time'] else 'inactive'
            })
        except Exception as e:
            logger.error(f"è·å–ETFæ•°æ®æƒ…å†µå¤±è´¥: {e}")
            collection_status.append({
                'type': 'ETFæ•°æ®',
                'category': 'etf_data',
                'icon': 'bi-pie-chart',
                'description': 'åŠ å¯†è´§å¸ETFèµ„é‡‘æµå‘æ•°æ®',
                'status': 'error',
                'error': str(e)
            })
        
        # 6. ä¼ä¸šé‡‘åº“æ•°æ®æƒ…å†µ
        try:
            cursor.execute("""
                SELECT 
                    COUNT(*) as count,
                    MAX(updated_at) as latest_time,
                    MIN(created_at) as oldest_time,
                    COUNT(DISTINCT company_id) as company_count
                FROM corporate_treasury_financing
            """)
            treasury_result = cursor.fetchone()
            collection_status.append({
                'type': 'ä¼ä¸šé‡‘åº“æ•°æ®',
                'category': 'treasury_data',
                'icon': 'bi-building',
                'description': 'ä¼ä¸šèèµ„è®°å½•æ•°æ®',
                'count': treasury_result['count'] if treasury_result else 0,
                'latest_time': treasury_result['latest_time'].isoformat() if treasury_result and treasury_result['latest_time'] else None,
                'oldest_time': treasury_result['oldest_time'].isoformat() if treasury_result and treasury_result['oldest_time'] else None,
                'company_count': treasury_result['company_count'] if treasury_result else 0,
                'status': 'active' if treasury_result and treasury_result['count'] > 0 else 'inactive'
            })
        except Exception as e:
            logger.error(f"è·å–ä¼ä¸šé‡‘åº“æ•°æ®æƒ…å†µå¤±è´¥: {e}")
            collection_status.append({
                'type': 'ä¼ä¸šé‡‘åº“æ•°æ®',
                'category': 'treasury_data',
                'icon': 'bi-building',
                'description': 'ä¼ä¸šèèµ„è®°å½•æ•°æ®',
                'status': 'error',
                'error': str(e)
            })
        
        # 7. Hyperliquidèªæ˜é’±æ•°æ®æƒ…å†µ
        try:
            # æ£€æŸ¥hyperliquid_wallet_tradesè¡¨
            cursor.execute("""
                SELECT 
                    COUNT(*) as count,
                    MAX(trade_time) as latest_time,
                    MIN(trade_time) as oldest_time,
                    COUNT(DISTINCT address) as wallet_count,
                    COUNT(DISTINCT coin) as coin_count
                FROM hyperliquid_wallet_trades
            """)
            hyperliquid_trades_result = cursor.fetchone()
            
            # æ£€æŸ¥hyperliquid_tradersè¡¨
            cursor.execute("""
                SELECT COUNT(*) as trader_count
                FROM hyperliquid_traders
            """)
            hyperliquid_traders_result = cursor.fetchone()
            
            # æ£€æŸ¥hyperliquid_monitored_walletsè¡¨
            cursor.execute("""
                SELECT COUNT(*) as monitored_count
                FROM hyperliquid_monitored_wallets
                WHERE is_monitoring = TRUE
            """)
            hyperliquid_monitored_result = cursor.fetchone()
            
            collection_status.append({
                'type': 'Hyperliquidèªæ˜é’±',
                'category': 'smart_money',
                'icon': 'bi-lightning-charge',
                'description': 'Hyperliquidå¹³å°èªæ˜é’±äº¤æ˜“æ•°æ®',
                'count': hyperliquid_trades_result['count'] if hyperliquid_trades_result else 0,
                'latest_time': hyperliquid_trades_result['latest_time'].isoformat() if hyperliquid_trades_result and hyperliquid_trades_result['latest_time'] else None,
                'oldest_time': hyperliquid_trades_result['oldest_time'].isoformat() if hyperliquid_trades_result and hyperliquid_trades_result['oldest_time'] else None,
                'wallet_count': hyperliquid_trades_result['wallet_count'] if hyperliquid_trades_result else 0,
                'trader_count': hyperliquid_traders_result['trader_count'] if hyperliquid_traders_result else 0,
                'monitored_count': hyperliquid_monitored_result['monitored_count'] if hyperliquid_monitored_result else 0,
                'coin_count': hyperliquid_trades_result['coin_count'] if hyperliquid_trades_result else 0,
                'status': _check_status_active(hyperliquid_trades_result['latest_time'], 86400) if hyperliquid_trades_result and hyperliquid_trades_result['latest_time'] else 'inactive'  # 24å°æ—¶é˜ˆå€¼
            })
        except Exception as e:
            logger.error(f"è·å–Hyperliquidèªæ˜é’±æ•°æ®æƒ…å†µå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            collection_status.append({
                'type': 'Hyperliquidèªæ˜é’±',
                'category': 'smart_money',
                'icon': 'bi-lightning-charge',
                'description': 'Hyperliquidå¹³å°èªæ˜é’±äº¤æ˜“æ•°æ®',
                'status': 'error',
                'error': str(e)
            })
        
        # 8. é“¾ä¸Šèªæ˜é’±æ•°æ®æƒ…å†µ
        try:
            # æ£€æŸ¥smart_money_transactionsè¡¨
            cursor.execute("""
                SELECT 
                    COUNT(*) as count,
                    MAX(timestamp) as latest_time,
                    MIN(timestamp) as oldest_time,
                    COUNT(DISTINCT address) as wallet_count,
                    COUNT(DISTINCT token_symbol) as token_count,
                    COUNT(DISTINCT blockchain) as blockchain_count
                FROM smart_money_transactions
            """)
            smart_money_tx_result = cursor.fetchone()
            
            # æ£€æŸ¥smart_money_signalsè¡¨
            cursor.execute("""
                SELECT 
                    COUNT(*) as signal_count,
                    MAX(timestamp) as latest_signal_time,
                    COUNT(DISTINCT token_symbol) as signal_token_count
                FROM smart_money_signals
                WHERE is_active = TRUE
            """)
            smart_money_signal_result = cursor.fetchone()
            
            # æ£€æŸ¥smart_money_addressesè¡¨
            cursor.execute("""
                SELECT COUNT(*) as address_count
                FROM smart_money_addresses
            """)
            smart_money_address_result = cursor.fetchone()
            
            collection_status.append({
                'type': 'é“¾ä¸Šèªæ˜é’±',
                'category': 'smart_money',
                'icon': 'bi-wallet2',
                'description': 'é“¾ä¸Šèªæ˜é’±äº¤æ˜“å’Œä¿¡å·æ•°æ®',
                'count': smart_money_tx_result['count'] if smart_money_tx_result else 0,
                'latest_time': smart_money_tx_result['latest_time'].isoformat() if smart_money_tx_result and smart_money_tx_result['latest_time'] else None,
                'oldest_time': smart_money_tx_result['oldest_time'].isoformat() if smart_money_tx_result and smart_money_tx_result['oldest_time'] else None,
                'wallet_count': smart_money_tx_result['wallet_count'] if smart_money_tx_result else 0,
                'address_count': smart_money_address_result['address_count'] if smart_money_address_result else 0,
                'token_count': smart_money_tx_result['token_count'] if smart_money_tx_result else 0,
                'blockchain_count': smart_money_tx_result['blockchain_count'] if smart_money_tx_result else 0,
                'signal_count': smart_money_signal_result['signal_count'] if smart_money_signal_result else 0,
                'latest_signal_time': smart_money_signal_result['latest_signal_time'].isoformat() if smart_money_signal_result and smart_money_signal_result['latest_signal_time'] else None,
                'status': _check_status_active(smart_money_tx_result['latest_time'], 86400) if smart_money_tx_result and smart_money_tx_result['latest_time'] else 'inactive'  # 24å°æ—¶é˜ˆå€¼
            })
        except Exception as e:
            logger.error(f"è·å–é“¾ä¸Šèªæ˜é’±æ•°æ®æƒ…å†µå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            collection_status.append({
                'type': 'é“¾ä¸Šèªæ˜é’±',
                'category': 'smart_money',
                'icon': 'bi-wallet2',
                'description': 'é“¾ä¸Šèªæ˜é’±äº¤æ˜“å’Œä¿¡å·æ•°æ®',
                'status': 'error',
                'error': str(e)
            })
        
        cursor.close()
        conn.close()
        
        return {
            'success': True,
            'data': collection_status
        }
        
    except Exception as e:
        logger.error(f"è·å–æ•°æ®é‡‡é›†æƒ…å†µå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–æ•°æ®é‡‡é›†æƒ…å†µå¤±è´¥: {str(e)}")


def _parse_date(date_str: str):
    """è§£ææ—¥æœŸå­—ç¬¦ä¸²ï¼Œæ”¯æŒå¤šç§æ ¼å¼"""
    if not date_str:
        return None
    try:
        # å°è¯•å¤šç§æ—¥æœŸæ ¼å¼
        for fmt in ['%Y-%m-%d', '%m/%d/%Y', '%d/%m/%Y', '%Y/%m/%d']:
            try:
                return datetime.strptime(str(date_str).strip(), fmt).date()
            except:
                continue
        return None
    except:
        return None

def _parse_number(num_str: str):
    """è§£ææ•°å­—å­—ç¬¦ä¸²ï¼Œæ”¯æŒé€—å·ã€è´§å¸ç¬¦å·ç­‰"""
    if not num_str:
        return None
    try:
        # ç§»é™¤é€—å·ã€è´§å¸ç¬¦å·ã€ç©ºæ ¼
        cleaned = str(num_str).replace(',', '').replace('$', '').replace(' ', '').strip()
        # å¤„ç†æ‹¬å·è¡¨ç¤ºè´Ÿæ•°çš„æƒ…å†µ
        is_negative = False
        if cleaned.startswith('(') and cleaned.endswith(')'):
            is_negative = True
            cleaned = cleaned[1:-1]
        if cleaned:
            value = float(cleaned)
            return -value if is_negative else value
        return None
    except:
        return None


@router.post("/import/etf")
async def import_etf_data(
    file: UploadFile = File(...),
    asset_type: str = Form("BTC")
):
    """
    å¯¼å…¥ETFæ•°æ®æ–‡ä»¶ï¼ˆCSVæ ¼å¼ï¼‰
    
    æ”¯æŒå¤šç§å­—æ®µåæ ¼å¼ï¼š
    - Date/date, Ticker/ticker
    - NetInflow/net_inflow, GrossInflow/gross_inflow, GrossOutflow/gross_outflow
    - AUM/aum, BTC_Holdings/BTCHoldings/btc_holdings, ETH_Holdings/ETHHoldings/eth_holdings
    - NAV/nav, Close/close_price, Volume/volume
    """
    try:
        if not file.filename.endswith('.csv'):
            raise HTTPException(status_code=400, detail="åªæ”¯æŒCSVæ–‡ä»¶")
        
        conn = get_db_connection()
        cursor = conn.cursor()
        
        # è¯»å–æ–‡ä»¶å†…å®¹
        content = await file.read()
        text_content = content.decode('utf-8-sig')  # å¤„ç†BOM
        reader = csv.DictReader(io.StringIO(text_content))
        
        imported = 0
        errors = []
        
        for row_num, row in enumerate(reader, start=2):  # ä»ç¬¬2è¡Œå¼€å§‹ï¼ˆç¬¬1è¡Œæ˜¯è¡¨å¤´ï¼‰
            try:
                # æ”¯æŒå¤šç§å­—æ®µåæ ¼å¼
                trade_date_str = row.get('Date') or row.get('date') or row.get('trade_date') or row.get('TradeDate')
                ticker = (row.get('Ticker') or row.get('ticker') or row.get('TICKER')).strip().upper() if (row.get('Ticker') or row.get('ticker') or row.get('TICKER')) else None
                
                if not ticker or not trade_date_str:
                    errors.append(f"ç¬¬{row_num}è¡Œ: ç¼ºå°‘å¿…è¦å­—æ®µ (Ticker/Date)")
                    continue
                
                # è§£ææ—¥æœŸï¼ˆæ”¯æŒå¤šç§æ ¼å¼ï¼‰
                trade_date = _parse_date(trade_date_str)
                if not trade_date:
                    errors.append(f"ç¬¬{row_num}è¡Œ: æ—¥æœŸæ ¼å¼é”™è¯¯ (æ”¯æŒ YYYY-MM-DD, MM/DD/YYYY ç­‰)")
                    continue
                
                # æŸ¥æ‰¾ETFäº§å“ï¼ˆè·å–IDå’Œèµ„äº§ç±»å‹ï¼‰
                cursor.execute("SELECT id, asset_type FROM crypto_etf_products WHERE ticker = %s", (ticker,))
                etf_result = cursor.fetchone()
                
                if not etf_result:
                    errors.append(f"ç¬¬{row_num}è¡Œ: æœªæ‰¾åˆ°ETFäº§å“ {ticker}ï¼Œè¯·å…ˆåœ¨ç³»ç»Ÿä¸­æ·»åŠ è¯¥ETF")
                    continue
                
                etf_id, db_asset_type = etf_result['id'], etf_result['asset_type']
                
                # è§£ææ•°å€¼å­—æ®µï¼ˆæ”¯æŒå¤šç§å­—æ®µåï¼‰
                net_inflow = _parse_number(row.get('NetInflow') or row.get('net_inflow') or row.get('Net_Inflow'))
                gross_inflow = _parse_number(row.get('GrossInflow') or row.get('gross_inflow') or row.get('Gross_Inflow'))
                gross_outflow = _parse_number(row.get('GrossOutflow') or row.get('gross_outflow') or row.get('Gross_Outflow'))
                aum = _parse_number(row.get('AUM') or row.get('aum'))
                
                # è§£ææŒä»“é‡ï¼ˆæ ¹æ®ETFèµ„äº§ç±»å‹ï¼‰
                btc_holdings = None
                eth_holdings = None
                if db_asset_type == 'BTC':
                    btc_holdings = _parse_number(
                        row.get('BTC_Holdings') or row.get('BTCHoldings') or 
                        row.get('btc_holdings') or row.get('Holdings') or row.get('holdings')
                    )
                elif db_asset_type == 'ETH':
                    eth_holdings = _parse_number(
                        row.get('ETH_Holdings') or row.get('ETHHoldings') or 
                        row.get('eth_holdings') or row.get('Holdings') or row.get('holdings')
                    )
                
                nav = _parse_number(row.get('NAV') or row.get('nav'))
                close_price = _parse_number(row.get('Close') or row.get('close_price') or row.get('ClosePrice'))
                volume = _parse_number(row.get('Volume') or row.get('volume'))
                data_source = (row.get('data_source') or row.get('DataSource') or 'manual').strip()
                
                # æ’å…¥æˆ–æ›´æ–°æ•°æ®
                cursor.execute("""
                    INSERT INTO crypto_etf_flows
                    (etf_id, ticker, trade_date, net_inflow, gross_inflow, gross_outflow,
                     aum, btc_holdings, eth_holdings, nav, close_price, volume, data_source)
                    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                    ON DUPLICATE KEY UPDATE
                        net_inflow = VALUES(net_inflow),
                        gross_inflow = VALUES(gross_inflow),
                        gross_outflow = VALUES(gross_outflow),
                        aum = VALUES(aum),
                        btc_holdings = VALUES(btc_holdings),
                        eth_holdings = VALUES(eth_holdings),
                        nav = VALUES(nav),
                        close_price = VALUES(close_price),
                        volume = VALUES(volume),
                        data_source = VALUES(data_source),
                        updated_at = CURRENT_TIMESTAMP
                """, (
                    etf_id, ticker, trade_date, net_inflow, gross_inflow, gross_outflow,
                    aum, btc_holdings, eth_holdings, nav, close_price, volume, data_source
                ))
                
                imported += 1
                
            except Exception as e:
                errors.append(f"ç¬¬{row_num}è¡Œ: {str(e)}")
                logger.error(f"å¯¼å…¥ETFæ•°æ®ç¬¬{row_num}è¡Œå¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
        
        conn.commit()
        cursor.close()
        conn.close()
        
        return {
            'success': True,
            'imported': imported,
            'errors': errors,
            'error_count': len(errors),
            'message': f'æˆåŠŸå¯¼å…¥ {imported} æ¡è®°å½•ï¼Œå¤±è´¥ {len(errors)} æ¡'
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"å¯¼å…¥ETFæ•°æ®å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"å¯¼å…¥ETFæ•°æ®å¤±è´¥: {str(e)}")


def parse_bitcoin_treasuries_format(text: str):
    """
    è§£æ Bitcoin Treasuries ç½‘ç«™çš„å¤åˆ¶æ ¼å¼
    
    å‚è€ƒ scripts/corporate_treasury/batch_import.py çš„ parse_bitcoin_treasuries_format å‡½æ•°
    
    ç¤ºä¾‹æ ¼å¼ï¼š
    1
    Strategy
    ğŸ‡ºğŸ‡¸	MSTR	640,808
    
    è¿”å›ï¼š[(å…¬å¸å, è‚¡ç¥¨ä»£ç , BTCæ•°é‡), ...]
    """
    companies = []
    lines = text.strip().split('\n')
    
    logger.debug(f"å¼€å§‹è§£æï¼Œå…± {len(lines)} è¡Œ")
    
    i = 0
    while i < len(lines):
        line = lines[i].strip()
        
        # è·³è¿‡æ³¨é‡Šè¡Œå’Œç©ºè¡Œ
        if line.startswith('#') or not line:
            i += 1
            continue
        
        # è·³è¿‡æ’åæ•°å­—
        if line.isdigit():
            i += 1
            continue
        
        # å¦‚æœæ˜¯å…¬å¸åï¼ˆä¸åŒ…å«åˆ¶è¡¨ç¬¦ï¼Œä¸”ä¸æ˜¯å›½æ——è¡Œï¼‰
        # æ³¨æ„ï¼šéœ€è¦æ’é™¤ä»¥å„ç§å›½æ——å¼€å¤´çš„è¡Œ
        if '\t' not in line and line:
            # æ£€æŸ¥æ˜¯å¦æ˜¯å›½æ——è¡Œï¼ˆå¯èƒ½åŒ…å«å„ç§å›½æ——emojiï¼‰
            is_flag_line = False
            for flag in ['ğŸ‡ºğŸ‡¸', 'ğŸ‡¯ğŸ‡µ', 'ğŸ‡¨ğŸ‡¦', 'ğŸ‡¬ğŸ‡§', 'ğŸ‡©ğŸ‡ª', 'ğŸ‡«ğŸ‡·', 'ğŸ‡¦ğŸ‡º', 'ğŸ‡¨ğŸ‡­', 'ğŸ‡¸ğŸ‡¬', 'ğŸ‡°ğŸ‡·']:
                if line.startswith(flag):
                    is_flag_line = True
                    break
            
            if not is_flag_line:
                company_name = line
                
                # ä¸‹ä¸€è¡Œåº”è¯¥æ˜¯å›½æ——ã€è‚¡ç¥¨ä»£ç å’Œæ•°é‡
                if i + 1 < len(lines):
                    next_line = lines[i + 1].strip()
                    
                    # è§£ææ ¼å¼ï¼šğŸ‡ºğŸ‡¸	MSTR	640,808
                    parts = next_line.split('\t')
                    
                    if len(parts) >= 3:
                        ticker = parts[1].strip()
                        btc_amount_str = parts[2].strip().replace(',', '')
                        
                        try:
                            btc_amount = float(btc_amount_str)
                            companies.append((company_name, ticker, btc_amount))
                            logger.debug(f"è§£ææˆåŠŸ: {company_name} ({ticker}) - {btc_amount:,.0f}")
                        except ValueError as e:
                            logger.warning(f"è·³è¿‡æ— æ•ˆæ•°é‡: {company_name} - {parts[2]} (é”™è¯¯: {e})")
                    else:
                        logger.warning(f"è·³è¿‡æ ¼å¼ä¸æ­£ç¡®çš„è¡Œ: {company_name} çš„ä¸‹ä¸€è¡Œæ ¼å¼é”™è¯¯: {next_line}")
                    
                    i += 2  # è·³è¿‡ä¸‹ä¸€è¡Œ
                    continue
        
        i += 1
    
    logger.info(f"è§£æå®Œæˆï¼Œå…±è§£æåˆ° {len(companies)} å®¶å…¬å¸")
    return companies


@router.post("/import/corporate-treasury")
async def import_corporate_treasury_data(
    file: UploadFile = File(...),
    asset_type: str = Form("BTC"),
    data_date: str = Form(None)
):
    """
    å¯¼å…¥ä¼ä¸šé‡‘åº“æ•°æ®æ–‡ä»¶
    
    æ”¯æŒä¸¤ç§æ ¼å¼ï¼š
    1. æ–‡æœ¬æ ¼å¼ï¼ˆ.txtï¼‰ï¼šBitcoin Treasuries ç½‘ç«™æ ¼å¼ï¼Œå¯¼å…¥æŒä»“æ•°æ®åˆ° corporate_treasury_purchases
    2. CSVæ ¼å¼ï¼ˆ.csvï¼‰ï¼šèèµ„æ•°æ®ï¼Œå¯¼å…¥åˆ° corporate_treasury_financing
    """
    try:
        conn = get_db_connection()
        # ä½¿ç”¨å­—å…¸æ¸¸æ ‡ï¼Œä¸ get_db_connection() è¿”å›çš„ DictCursor ä¸€è‡´
        cursor = conn.cursor(pymysql.cursors.DictCursor)
        
        # è¯»å–æ–‡ä»¶å†…å®¹
        content = await file.read()
        text_content = content.decode('utf-8')
        
        # åˆ¤æ–­æ–‡ä»¶ç±»å‹
        is_text_format = file.filename.endswith('.txt')
        
        imported = 0
        errors = []
        
        if is_text_format:
            # æ–‡æœ¬æ ¼å¼ï¼šBitcoin Treasuries æ ¼å¼ï¼Œå¯¼å…¥æŒä»“æ•°æ®
            # è§£ææ•°æ®æ—¥æœŸ
            purchase_date = None
            if data_date:
                try:
                    purchase_date = datetime.strptime(data_date, '%Y-%m-%d').date()
                except:
                    raise HTTPException(status_code=400, detail="æ•°æ®æ—¥æœŸæ ¼å¼é”™è¯¯ (åº”ä¸º YYYY-MM-DD)")
            else:
                purchase_date = datetime.now().date()
            
            # è§£ææ–‡æœ¬æ ¼å¼
            companies = parse_bitcoin_treasuries_format(text_content)
            
            logger.info(f"è§£æåˆ° {len(companies)} å®¶å…¬å¸")
            if companies:
                logger.info(f"å‰3å®¶å…¬å¸: {companies[:3]}")
            
            if not companies:
                raise HTTPException(status_code=400, detail="æ— æ³•è§£ææ–‡æœ¬æ ¼å¼ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼æ˜¯å¦æ­£ç¡®ã€‚ç¡®ä¿æ–‡ä»¶æ ¼å¼ä¸ºï¼šæ’åæ•°å­—ã€å…¬å¸åã€å›½æ——+è‚¡ç¥¨ä»£ç +æŒä»“é‡ï¼ˆç”¨åˆ¶è¡¨ç¬¦åˆ†éš”ï¼‰")
            
            # å¯¼å…¥æŒä»“æ•°æ®ï¼ˆå‚è€ƒ batch_import.py çš„ import_companies é€»è¾‘ï¼‰
            for company_name, ticker, holdings in companies:
                try:
                    # 1. æŸ¥æ‰¾æˆ–åˆ›å»ºå…¬å¸
                    cursor.execute("""
                        SELECT id FROM corporate_treasury_companies
                        WHERE company_name = %s OR ticker_symbol = %s
                        LIMIT 1
                    """, (company_name, ticker))
                    company_result = cursor.fetchone()
                    
                    if not company_result:
                        # åˆ›å»ºæ–°å…¬å¸
                        cursor.execute("""
                            INSERT INTO corporate_treasury_companies
                            (company_name, ticker_symbol, category, is_active)
                            VALUES (%s, %s, %s, 1)
                        """, (company_name, ticker, 'holding'))
                        company_id = cursor.lastrowid
                        logger.info(f"æ–°å¢å…¬å¸: {company_name} ({ticker})")
                    else:
                        # ä½¿ç”¨å­—å…¸æ¸¸æ ‡ï¼Œé€šè¿‡é”®è®¿é—®
                        company_id = company_result['id'] if isinstance(company_result, dict) else company_result[0]
                    
                    # 2. æ£€æŸ¥æ˜¯å¦å·²æœ‰è¯¥æ—¥æœŸçš„è®°å½•
                    cursor.execute("""
                        SELECT id, cumulative_holdings FROM corporate_treasury_purchases
                        WHERE company_id = %s AND purchase_date = %s AND asset_type = %s
                    """, (company_id, purchase_date, asset_type))
                    existing = cursor.fetchone()
                    
                    if existing:
                        # ä½¿ç”¨å­—å…¸æ¸¸æ ‡ï¼Œé€šè¿‡é”®è®¿é—®
                        existing_id = existing['id'] if isinstance(existing, dict) else existing[0]
                        existing_holdings = existing['cumulative_holdings'] if isinstance(existing, dict) else existing[1]
                        # å¦‚æœæŒä»“é‡ç›¸åŒï¼Œè·³è¿‡
                        if existing_holdings and float(existing_holdings) == holdings:
                            logger.debug(f"è·³è¿‡ï¼ˆå·²å­˜åœ¨ï¼‰: {company_name} - {holdings:,.0f} {asset_type}")
                            continue
                        
                        # æ›´æ–°è®°å½•
                        cursor.execute("""
                            UPDATE corporate_treasury_purchases
                            SET cumulative_holdings = %s, updated_at = CURRENT_TIMESTAMP
                            WHERE id = %s
                        """, (holdings, existing_id))
                        logger.info(f"æ›´æ–°: {company_name} ({ticker}) - {holdings:,.0f} {asset_type}")
                    else:
                        # 3. è·å–ä¸Šä¸€æ¬¡çš„æŒä»“é‡ï¼ˆè®¡ç®—è´­ä¹°æ•°é‡ï¼‰
                        cursor.execute("""
                            SELECT cumulative_holdings FROM corporate_treasury_purchases
                            WHERE company_id = %s AND asset_type = %s
                            ORDER BY purchase_date DESC
                            LIMIT 1
                        """, (company_id, asset_type))
                        last_record = cursor.fetchone()
                        # ä½¿ç”¨å­—å…¸æ¸¸æ ‡ï¼Œé€šè¿‡é”®è®¿é—®
                        if last_record:
                            if isinstance(last_record, dict):
                                last_holdings = float(last_record.get('cumulative_holdings', 0) or 0)
                            else:
                                last_holdings = float(last_record[0] or 0)
                        else:
                            last_holdings = 0
                        
                        # è®¡ç®—è´­ä¹°æ•°é‡
                        quantity = holdings - last_holdings
                        
                        # æ’å…¥æ–°è®°å½•
                        cursor.execute("""
                            INSERT INTO corporate_treasury_purchases
                            (company_id, purchase_date, asset_type, quantity, cumulative_holdings, data_source)
                            VALUES (%s, %s, %s, %s, %s, %s)
                        """, (company_id, purchase_date, asset_type, quantity, holdings, 'manual'))
                        logger.info(f"æ–°å¢: {company_name} ({ticker}) - {quantity:+,.0f} {asset_type} â†’ {holdings:,.0f}")
                    
                    imported += 1
                    
                except Exception as e:
                    error_msg = f"{company_name} ({ticker}): {str(e)}"
                    errors.append(error_msg)
                    logger.error(f"å¯¼å…¥ä¼ä¸šé‡‘åº“æŒä»“æ•°æ®å¤±è´¥: {error_msg}")
                    import traceback
                    logger.error(traceback.format_exc())
            
            message = f'æˆåŠŸå¯¼å…¥ {imported} æ¡æŒä»“è®°å½•ï¼Œå¤±è´¥ {len(errors)} æ¡'
            
        else:
            # CSVæ ¼å¼ï¼šèèµ„æ•°æ®
            if not file.filename.endswith('.csv'):
                raise HTTPException(status_code=400, detail="åªæ”¯æŒ .txt æˆ– .csv æ–‡ä»¶")
            
            # è§£ææ•°æ®æ—¥æœŸ
            financing_date = None
            if data_date:
                try:
                    financing_date = datetime.strptime(data_date, '%Y-%m-%d').date()
                except:
                    raise HTTPException(status_code=400, detail="æ•°æ®æ—¥æœŸæ ¼å¼é”™è¯¯ (åº”ä¸º YYYY-MM-DD)")
            else:
                financing_date = datetime.now().date()
            
            # è¯»å–CSVå†…å®¹
            text_content_bom = content.decode('utf-8-sig')  # å¤„ç†BOM
            reader = csv.DictReader(io.StringIO(text_content_bom))
            
            for row_num, row in enumerate(reader, start=2):
                try:
                    company_name = row.get('company_name', '').strip()
                    ticker = (row.get('ticker') or row.get('ticker_symbol') or '').strip().upper()
                    
                    if not company_name:
                        errors.append(f"ç¬¬{row_num}è¡Œ: ç¼ºå°‘å…¬å¸åç§°")
                        continue
                    
                    # æŸ¥æ‰¾æˆ–åˆ›å»ºå…¬å¸
                    cursor.execute("""
                        SELECT id FROM corporate_treasury_companies
                        WHERE company_name = %s OR ticker_symbol = %s
                        LIMIT 1
                    """, (company_name, ticker))
                    company_result = cursor.fetchone()
                    
                    if not company_result:
                        cursor.execute("""
                            INSERT INTO corporate_treasury_companies
                            (company_name, ticker_symbol, category, is_active)
                            VALUES (%s, %s, %s, 1)
                        """, (company_name, ticker, 'holding'))
                        company_id = cursor.lastrowid
                    else:
                        company_id = company_result['id']
                    
                    # è§£æèèµ„æ—¥æœŸ
                    row_financing_date = financing_date
                    financing_date_str = row.get('financing_date', '').strip()
                    if financing_date_str:
                        parsed_date = _parse_date(financing_date_str)
                        if parsed_date:
                            row_financing_date = parsed_date
                    
                    # è§£ææ•°å€¼å­—æ®µ
                    financing_type = (row.get('financing_type') or 'equity').strip() or 'equity'
                    amount = _parse_number(row.get('amount'))
                    purpose = row.get('purpose', '').strip() or None
                    announcement_url = row.get('announcement_url', '').strip() or None
                    notes = row.get('notes', '').strip() or None
                    data_source = (row.get('data_source') or 'manual').strip()
                    
                    # æ’å…¥èèµ„è®°å½•
                    cursor.execute("""
                        INSERT INTO corporate_treasury_financing
                        (company_id, financing_date, financing_type, amount, purpose, announcement_url, notes, data_source)
                        VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
                    """, (company_id, row_financing_date, financing_type, amount, purpose, announcement_url, notes, data_source))
                    
                    imported += 1
                    
                except Exception as e:
                    errors.append(f"ç¬¬{row_num}è¡Œ: {str(e)}")
                    logger.error(f"å¯¼å…¥ä¼ä¸šé‡‘åº“èèµ„æ•°æ®ç¬¬{row_num}è¡Œå¤±è´¥: {e}")
            
            message = f'æˆåŠŸå¯¼å…¥ {imported} æ¡èèµ„è®°å½•ï¼Œå¤±è´¥ {len(errors)} æ¡'
        
        conn.commit()
        cursor.close()
        conn.close()
        
        return {
            'success': True,
            'imported': imported,
            'errors': errors,
            'error_count': len(errors),
            'message': message
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"å¯¼å…¥ä¼ä¸šé‡‘åº“æ•°æ®å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"å¯¼å…¥ä¼ä¸šé‡‘åº“æ•°æ®å¤±è´¥: {str(e)}")


@router.get("/template/etf")
async def download_etf_template():
    """
    ä¸‹è½½ETFæ•°æ®å¯¼å…¥æ¨¡æ¿CSVæ–‡ä»¶
    
    å‚è€ƒ btc_etf_2025-11-08.csv çš„æ ¼å¼
    æ ¼å¼ï¼šDate, Ticker, NetInflow, BTC_Holdings
    """
    try:
        # åˆ›å»ºæ¨¡æ¿å†…å®¹ï¼ˆå‚è€ƒ btc_etf_2025-11-08.csv çš„æ ¼å¼ï¼‰
        # æ ¼å¼ï¼šDate, Ticker, NetInflow, BTC_Holdings
        # NetInflow æ˜¯ç¾å…ƒé‡‘é¢ï¼ˆä¸æ˜¯ç™¾ä¸‡ç¾å…ƒï¼‰ï¼ŒBTC_Holdings æ˜¯æŒä»“é‡
        from datetime import date, timedelta
        yesterday = (date.today() - timedelta(days=1)).strftime('%Y-%m-%d')
        
        template_content = "Date,Ticker,NetInflow,BTC_Holdings\n"
        template_content += f"{yesterday},IBIT,0,0\n"
        template_content += f"{yesterday},FBTC,0,0\n"
        template_content += f"{yesterday},BITB,0,0\n"
        template_content += f"{yesterday},ARKB,0,0\n"
        template_content += f"{yesterday},BTCO,0,0\n"
        template_content += f"{yesterday},EZBC,0,0\n"
        template_content += f"{yesterday},BRRR,0,0\n"
        template_content += f"{yesterday},HODL,0,0\n"
        template_content += f"{yesterday},BTCW,0,0\n"
        template_content += f"{yesterday},GBTC,0,0\n"
        template_content += f"{yesterday},DEFI,0,0\n"
        
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
        template_path = Path(__file__).parent.parent.parent / "templates" / "etf_import_template.csv"
        template_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(template_path, 'w', encoding='utf-8-sig', newline='') as f:
            f.write(template_content)
        
        return FileResponse(
            path=str(template_path),
            filename="etf_import_template.csv",
            media_type="text/csv"
        )
        
    except Exception as e:
        logger.error(f"ç”ŸæˆETFæ¨¡æ¿å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"ç”ŸæˆETFæ¨¡æ¿å¤±è´¥: {str(e)}")


@router.get("/template/corporate-treasury")
async def download_corporate_treasury_template():
    """
    ä¸‹è½½ä¼ä¸šé‡‘åº“æŒä»“æ•°æ®å¯¼å…¥æ¨¡æ¿ï¼ˆæ–‡æœ¬æ ¼å¼ï¼‰
    
    å‚è€ƒ scripts/corporate_treasury/import_template.txt
    æ ¼å¼ï¼šä» Bitcoin Treasuries ç½‘ç«™å¤åˆ¶çš„æ ¼å¼
    """
    try:
        # åˆ›å»ºæ¨¡æ¿å†…å®¹ï¼ˆå‚è€ƒ scripts/corporate_treasury/import_template.txtï¼‰
        template_content = """# ä¼ä¸šé‡‘åº“æ‰¹é‡å¯¼å…¥æ¨¡æ¿
# ä» Bitcoin Treasuries ç½‘ç«™å¤åˆ¶çš„æ ¼å¼ç¤ºä¾‹
# ä½¿ç”¨æ–¹æ³•ï¼šå¤åˆ¶ä»¥ä¸‹å†…å®¹ï¼Œæˆ–ä» https://bitcointreasuries.net/ å¤åˆ¶æœ€æ–°æ•°æ®

1
Strategy
ğŸ‡ºğŸ‡¸	MSTR	640,808
2
MARA Holdings, Inc.
ğŸ‡ºğŸ‡¸	MARA	53,250
3
XXI
ğŸ‡ºğŸ‡¸	CEP	43,514
4
Metaplanet Inc.
ğŸ‡¯ğŸ‡µ	MTPLF	30,823
5
Bitcoin Standard Treasury Company
ğŸ‡ºğŸ‡¸	CEPO	30,021
6
Riot Platforms, Inc.
ğŸ‡ºğŸ‡¸	RIOT	19,287
7
Tesla, Inc.
ğŸ‡ºğŸ‡¸	TSLA	11,509
8
Coinbase Global, Inc.
ğŸ‡ºğŸ‡¸	COIN	11,776
9
Block, Inc.
ğŸ‡ºğŸ‡¸	SQ	8,692
10
Galaxy Digital Holdings Ltd
ğŸ‡ºğŸ‡¸	GLXY	6,894
"""
        
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
        template_path = Path(__file__).parent.parent.parent / "templates" / "corporate_treasury_import_template.txt"
        template_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(template_path, 'w', encoding='utf-8', newline='') as f:
            f.write(template_content)
        
        return FileResponse(
            path=str(template_path),
            filename="corporate_treasury_import_template.txt",
            media_type="text/plain"
        )
        
    except Exception as e:
        logger.error(f"ç”Ÿæˆä¼ä¸šé‡‘åº“æ¨¡æ¿å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"ç”Ÿæˆä¼ä¸šé‡‘åº“æ¨¡æ¿å¤±è´¥: {str(e)}")


@router.get("/template/corporate-treasury-financing")
async def download_corporate_treasury_financing_template():
    """
    ä¸‹è½½ä¼ä¸šé‡‘åº“èèµ„æ•°æ®å¯¼å…¥æ¨¡æ¿CSVæ–‡ä»¶
    
    ç”¨äºå¯¼å…¥ corporate_treasury_financing è¡¨
    """
    try:
        # åˆ›å»ºæ¨¡æ¿å†…å®¹ï¼ˆèèµ„æ•°æ®CSVæ ¼å¼ï¼‰
        template_content = "company_name,ticker,financing_date,financing_type,amount,purpose,announcement_url,notes,data_source\n"
        template_content += "MicroStrategy,MSTR,2025-01-27,equity,1000000,è´­ä¹°BTC,https://example.com/announcement1,èèµ„ç”¨äºè´­ä¹°BTC,manual\n"
        template_content += "Tesla,TSLA,2025-01-27,convertible_note,500000,è´­ä¹°BTC,https://example.com/announcement2,å¯è½¬æ¢å€ºåˆ¸,manual\n"
        template_content += "Block,SQ,2025-01-27,loan,300000,è´­ä¹°BTC,https://example.com/announcement3,è´·æ¬¾èèµ„,manual\n"
        template_content += "Coinbase,COIN,2025-01-27,atm,200000,è´­ä¹°BTC,https://example.com/announcement4,ATMèèµ„,manual\n"
        
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
        template_path = Path(__file__).parent.parent.parent / "templates" / "corporate_treasury_financing_import_template.csv"
        template_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(template_path, 'w', encoding='utf-8-sig', newline='') as f:
            f.write(template_content)
        
        return FileResponse(
            path=str(template_path),
            filename="corporate_treasury_financing_import_template.csv",
            media_type="text/csv"
        )
        
    except Exception as e:
        logger.error(f"ç”Ÿæˆä¼ä¸šé‡‘åº“èèµ„æ¨¡æ¿å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"ç”Ÿæˆä¼ä¸šé‡‘åº“èèµ„æ¨¡æ¿å¤±è´¥: {str(e)}")


@router.post("/collect")
async def collect_historical_data(request: Dict):
    """
    é‡‡é›†å†å²æ•°æ®
    
    Args:
        request: åŒ…å«ä»¥ä¸‹å­—æ®µçš„å­—å…¸
            - symbols: äº¤æ˜“å¯¹åˆ—è¡¨ï¼Œå¦‚ ["BTC/USDT", "ETH/USDT"]
            - data_type: æ•°æ®ç±»å‹ï¼Œ'price'ã€'kline' æˆ– 'both'ï¼ˆåŒæ—¶é‡‡é›†ä»·æ ¼å’ŒKçº¿æ•°æ®ï¼‰
            - start_time: å¼€å§‹æ—¶é—´ (ISOæ ¼å¼å­—ç¬¦ä¸²)
            - end_time: ç»“æŸæ—¶é—´ (ISOæ ¼å¼å­—ç¬¦ä¸²)
            - timeframes: æ—¶é—´å‘¨æœŸåˆ—è¡¨ï¼ˆä»…Kçº¿æ•°æ®éœ€è¦ï¼‰ï¼Œå¦‚ ['1m', '5m', '1h']ï¼Œé»˜è®¤ ['1m', '5m', '15m', '1h', '1d']
            - timeframe: å•ä¸ªæ—¶é—´å‘¨æœŸï¼ˆå‘åå…¼å®¹ï¼Œå¦‚æœtimeframesä¸å­˜åœ¨åˆ™ä½¿ç”¨æ­¤å­—æ®µï¼‰
            - collect_futures: æ˜¯å¦é‡‡é›†åˆçº¦æ•°æ® (bool)
            - save_to_config: æ˜¯å¦ä¿å­˜åˆ°é…ç½®æ–‡ä»¶ (bool)
    """
    try:
        symbols = request.get('symbols', [])
        data_type = request.get('data_type', 'price')
        start_time_str = request.get('start_time')
        end_time_str = request.get('end_time')
        # æ”¯æŒå¤šä¸ªæ—¶é—´å‘¨æœŸï¼Œé»˜è®¤æ‰€æœ‰å‘¨æœŸ
        timeframes = request.get('timeframes', None)
        if not timeframes:
            # å‘åå…¼å®¹ï¼šå¦‚æœåªæœ‰å•ä¸ªtimeframeï¼Œè½¬æ¢ä¸ºåˆ—è¡¨
            timeframe = request.get('timeframe', '1h')
            timeframes = [timeframe] if data_type == 'kline' else []
        collect_futures = request.get('collect_futures', False)
        save_to_config = request.get('save_to_config', False)
        
        if not symbols:
            raise HTTPException(status_code=400, detail="äº¤æ˜“å¯¹åˆ—è¡¨ä¸èƒ½ä¸ºç©º")
        
        if not start_time_str or not end_time_str:
            raise HTTPException(status_code=400, detail="å¼€å§‹æ—¶é—´å’Œç»“æŸæ—¶é—´ä¸èƒ½ä¸ºç©º")
        
        # è§£ææ—¶é—´
        try:
            start_time = datetime.fromisoformat(start_time_str.replace('Z', '+00:00'))
            end_time = datetime.fromisoformat(end_time_str.replace('Z', '+00:00'))
        except Exception as e:
            raise HTTPException(status_code=400, detail=f"æ—¶é—´æ ¼å¼é”™è¯¯: {str(e)}")
        
        if start_time >= end_time:
            raise HTTPException(status_code=400, detail="ç»“æŸæ—¶é—´å¿…é¡»æ™šäºå¼€å§‹æ—¶é—´")
        
        # å¯¼å…¥é‡‡é›†å™¨
        from app.collectors.price_collector import MultiExchangeCollector
        import yaml
        from pathlib import Path
        
        # åŠ è½½é…ç½®
        config_path = Path(__file__).parent.parent.parent / "config.yaml"
        if not config_path.exists():
            raise HTTPException(status_code=500, detail="é…ç½®æ–‡ä»¶ä¸å­˜åœ¨")
        
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        # åˆå§‹åŒ–é‡‡é›†å™¨
        collector = MultiExchangeCollector(config)
        
        # åˆå§‹åŒ–åˆçº¦é‡‡é›†å™¨ï¼ˆå¦‚æœéœ€è¦ï¼‰
        futures_collector = None
        if collect_futures:
            try:
                from app.collectors.binance_futures_collector import BinanceFuturesCollector
                binance_config = config.get('exchanges', {}).get('binance', {})
                futures_collector = BinanceFuturesCollector(binance_config)
                logger.info("åˆçº¦æ•°æ®é‡‡é›†å™¨åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                logger.warning(f"åˆçº¦æ•°æ®é‡‡é›†å™¨åˆå§‹åŒ–å¤±è´¥: {e}ï¼Œå°†è·³è¿‡åˆçº¦æ•°æ®é‡‡é›†")
                collect_futures = False
        
        conn = get_db_connection()
        cursor = conn.cursor()
        
        total_saved = 0
        errors = []
        
        # éå†æ¯ä¸ªäº¤æ˜“å¯¹
        for symbol in symbols:
            try:
                symbol = symbol.strip()
                if not symbol:
                    continue
                
                logger.info(f"å¼€å§‹é‡‡é›† {symbol} çš„{data_type}æ•°æ®ï¼Œæ—¶é—´èŒƒå›´: {start_time} - {end_time}")
                
                # åˆ¤æ–­æ˜¯å¦éœ€è¦é‡‡é›†ä»·æ ¼æ•°æ®
                collect_price = data_type in ['price', 'both']
                # åˆ¤æ–­æ˜¯å¦éœ€è¦é‡‡é›†Kçº¿æ•°æ®
                collect_kline = data_type in ['kline', 'both']
                
                if collect_price:
                    # é‡‡é›†ä»·æ ¼æ•°æ® - ä½¿ç”¨1åˆ†é’ŸKçº¿æ•°æ®æ¥è·å–å†å²ä»·æ ¼
                    df = await collector.fetch_historical_data(
                        symbol=symbol,
                        timeframe='1m',
                        days=int((end_time - start_time).total_seconds() / 86400) + 1
                    )
                    
                    if df is not None and len(df) > 0:
                        # è¿‡æ»¤æ—¶é—´èŒƒå›´
                        df = df[(df['timestamp'] >= start_time) & (df['timestamp'] <= end_time)]
                        
                        saved_count = 0
                        for _, row in df.iterrows():
                            try:
                                # è·å–å½“å‰æ—¶é—´ä½œä¸ºcreated_at
                                created_at = datetime.now()
                                
                                # ä»Kçº¿æ•°æ®è½¬æ¢ä¸ºä»·æ ¼æ•°æ®æ ¼å¼
                                cursor.execute("""
                                    INSERT INTO price_data
                                    (symbol, exchange, timestamp, price, open_price, high_price, low_price, close_price, volume, quote_volume, bid_price, ask_price, change_24h, created_at)
                                    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                                    ON DUPLICATE KEY UPDATE
                                        price = VALUES(price),
                                        open_price = VALUES(open_price),
                                        high_price = VALUES(high_price),
                                        low_price = VALUES(low_price),
                                        close_price = VALUES(close_price),
                                        volume = VALUES(volume),
                                        quote_volume = VALUES(quote_volume),
                                        bid_price = VALUES(bid_price),
                                        ask_price = VALUES(ask_price),
                                        change_24h = VALUES(change_24h),
                                        created_at = VALUES(created_at)
                                """, (
                                    symbol,
                                    'binance',  # é»˜è®¤äº¤æ˜“æ‰€
                                    row['timestamp'],
                                    float(row['close']),  # ä½¿ç”¨æ”¶ç›˜ä»·ä½œä¸ºä»·æ ¼
                                    float(row['open']),
                                    float(row['high']),
                                    float(row['low']),
                                    float(row['close']),
                                    float(row['volume']),
                                    float(row.get('quote_volume', 0)),
                                    0,  # bid
                                    0,  # ask
                                    0,  # change_24h (å†å²æ•°æ®æ— æ³•è®¡ç®—)
                                    created_at
                                ))
                                saved_count += 1
                            except Exception as e:
                                logger.error(f"ä¿å­˜ä»·æ ¼æ•°æ®å¤±è´¥: {e}")
                                continue
                        
                        total_saved += saved_count
                        logger.info(f"{symbol} ä»·æ ¼æ•°æ®é‡‡é›†å®Œæˆï¼Œä¿å­˜ {saved_count} æ¡")
                    else:
                        errors.append(f"{symbol}: æœªè·å–åˆ°ä»·æ ¼æ•°æ®")
                
                if collect_kline:
                    # é‡‡é›†Kçº¿æ•°æ® - å¯¹æ‰€æœ‰æ—¶é—´å‘¨æœŸè¿›è¡Œé‡‡é›†
                    if not timeframes:
                        timeframes = ['1m', '5m', '15m', '1h', '1d']  # é»˜è®¤æ‰€æœ‰å‘¨æœŸ
                    
                    symbol_saved = 0
                    for timeframe in timeframes:
                        try:
                            logger.info(f"  é‡‡é›† {symbol} {timeframe} Kçº¿æ•°æ®...")
                            # ä½¿ç”¨å†å²æ•°æ®é‡‡é›†æ–¹æ³•
                            df = await collector.fetch_historical_data(
                                symbol=symbol,
                                timeframe=timeframe,
                                days=int((end_time - start_time).total_seconds() / 86400) + 1
                            )
                            
                            if df is not None and len(df) > 0:
                                # è¿‡æ»¤æ—¶é—´èŒƒå›´
                                df = df[(df['timestamp'] >= start_time) & (df['timestamp'] <= end_time)]
                                
                                timeframe_saved = 0
                                for _, row in df.iterrows():
                                    try:
                                        # è®¡ç®—æ—¶é—´æˆ³ï¼ˆæ¯«ç§’ï¼‰
                                        timestamp = row['timestamp']
                                        # ç¡®ä¿timestampæ˜¯datetimeç±»å‹
                                        if isinstance(timestamp, pd.Timestamp):
                                            timestamp_dt = timestamp.to_pydatetime()
                                            open_time_ms = int(timestamp.timestamp() * 1000)
                                        elif isinstance(timestamp, datetime):
                                            timestamp_dt = timestamp
                                            open_time_ms = int(timestamp.timestamp() * 1000)
                                        else:
                                            # å°è¯•è½¬æ¢
                                            timestamp_dt = pd.to_datetime(timestamp).to_pydatetime()
                                            open_time_ms = int(pd.to_datetime(timestamp).timestamp() * 1000)
                                        
                                        # è®¡ç®—æ”¶ç›˜æ—¶é—´ï¼ˆæ ¹æ®æ—¶é—´å‘¨æœŸï¼‰
                                        timeframe_minutes = {
                                            '1m': 1, '5m': 5, '15m': 15, '30m': 30,
                                            '1h': 60, '4h': 240, '1d': 1440
                                        }.get(timeframe, 60)
                                        close_time_ms = open_time_ms + (timeframe_minutes * 60 * 1000) - 1
                                        
                                        # è·å–å½“å‰æ—¶é—´ä½œä¸ºcreated_at
                                        created_at = datetime.now()
                                        
                                        cursor.execute("""
                                            INSERT INTO kline_data
                                            (symbol, exchange, timeframe, open_time, close_time, timestamp, open_price, high_price, low_price, close_price, volume, quote_volume, created_at)
                                            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                                            ON DUPLICATE KEY UPDATE
                                                open_price = VALUES(open_price),
                                                high_price = VALUES(high_price),
                                                low_price = VALUES(low_price),
                                                close_price = VALUES(close_price),
                                                volume = VALUES(volume),
                                                quote_volume = VALUES(quote_volume),
                                                created_at = VALUES(created_at)
                                        """, (
                                            symbol,
                                            'binance',  # é»˜è®¤äº¤æ˜“æ‰€
                                            timeframe,
                                            open_time_ms,
                                            close_time_ms,
                                            timestamp_dt,
                                            float(row['open']),
                                            float(row['high']),
                                            float(row['low']),
                                            float(row['close']),
                                            float(row['volume']),
                                            float(row.get('quote_volume', 0)),
                                            created_at
                                        ))
                                        timeframe_saved += 1
                                    except Exception as e:
                                        logger.error(f"ä¿å­˜Kçº¿æ•°æ®å¤±è´¥ ({timeframe}): {e}")
                                        continue
                                
                                symbol_saved += timeframe_saved
                                logger.info(f"  âœ“ {symbol} {timeframe} Kçº¿æ•°æ®é‡‡é›†å®Œæˆï¼Œä¿å­˜ {timeframe_saved} æ¡")
                            else:
                                logger.warning(f"  âŠ— {symbol} {timeframe}: æœªè·å–åˆ°Kçº¿æ•°æ®")
                        except Exception as e:
                            error_msg = f"{symbol} {timeframe}: {str(e)}"
                            errors.append(error_msg)
                            logger.error(f"é‡‡é›† {symbol} {timeframe} Kçº¿æ•°æ®å¤±è´¥: {e}")
                    
                    total_saved += symbol_saved
                    if symbol_saved > 0:
                        logger.info(f"{symbol} Kçº¿æ•°æ®é‡‡é›†å®Œæˆï¼Œå…±ä¿å­˜ {symbol_saved} æ¡ï¼ˆæ‰€æœ‰å‘¨æœŸï¼‰")
                    else:
                        errors.append(f"{symbol}: æ‰€æœ‰å‘¨æœŸå‡æœªè·å–åˆ°Kçº¿æ•°æ®")
                
                # é‡‡é›†åˆçº¦æ•°æ®
                if collect_futures and futures_collector:
                    try:
                        logger.info(f"å¼€å§‹é‡‡é›† {symbol} çš„åˆçº¦æ•°æ®...")
                        futures_saved = 0
                        
                        # å¯¹æ¯ä¸ªæ—¶é—´å‘¨æœŸé‡‡é›†åˆçº¦Kçº¿æ•°æ®
                        for timeframe in timeframes:
                            try:
                                logger.info(f"  é‡‡é›† {symbol} åˆçº¦ {timeframe} Kçº¿æ•°æ®...")
                                
                                # è®¡ç®—éœ€è¦è·å–çš„æ•°æ®é‡
                                days = int((end_time - start_time).total_seconds() / 86400) + 1
                                # æ ¹æ®æ—¶é—´å‘¨æœŸè®¡ç®—limit
                                timeframe_minutes = {
                                    '1m': 1, '5m': 5, '15m': 15, '30m': 30,
                                    '1h': 60, '4h': 240, '1d': 1440
                                }.get(timeframe, 60)
                                # æ¯ä¸ªå‘¨æœŸéœ€è¦çš„Kçº¿æ•°é‡
                                klines_needed = int(days * 1440 / timeframe_minutes)
                                limit = min(klines_needed, 1500)  # å¸å®‰é™åˆ¶æœ€å¤§1500
                                
                                # è·å–åˆçº¦Kçº¿æ•°æ®
                                df = await futures_collector.fetch_futures_klines(
                                    symbol=symbol,
                                    timeframe=timeframe,
                                    limit=limit
                                )
                                
                                if df is not None and len(df) > 0:
                                    # è¿‡æ»¤æ—¶é—´èŒƒå›´
                                    df = df[(df['timestamp'] >= start_time) & (df['timestamp'] <= end_time)]
                                    
                                    timeframe_saved = 0
                                    for _, row in df.iterrows():
                                        try:
                                            # è®¡ç®—æ—¶é—´æˆ³ï¼ˆæ¯«ç§’ï¼‰
                                            timestamp = row['timestamp']
                                            if isinstance(timestamp, pd.Timestamp):
                                                timestamp_dt = timestamp.to_pydatetime()
                                                open_time_ms = int(timestamp.timestamp() * 1000)
                                            elif isinstance(timestamp, datetime):
                                                timestamp_dt = timestamp
                                                open_time_ms = int(timestamp.timestamp() * 1000)
                                            else:
                                                timestamp_dt = pd.to_datetime(timestamp).to_pydatetime()
                                                open_time_ms = int(pd.to_datetime(timestamp).timestamp() * 1000)
                                            
                                            # è®¡ç®—æ”¶ç›˜æ—¶é—´
                                            timeframe_minutes = {
                                                '1m': 1, '5m': 5, '15m': 15, '30m': 30,
                                                '1h': 60, '4h': 240, '1d': 1440
                                            }.get(timeframe, 60)
                                            close_time_ms = open_time_ms + (timeframe_minutes * 60 * 1000) - 1
                                            
                                            # è·å–å½“å‰æ—¶é—´ä½œä¸ºcreated_at
                                            created_at = datetime.now()
                                            
                                            # ä¿å­˜åˆçº¦Kçº¿æ•°æ®
                                            cursor.execute("""
                                                INSERT INTO kline_data
                                                (symbol, exchange, timeframe, open_time, close_time, timestamp, open_price, high_price, low_price, close_price, volume, quote_volume, created_at)
                                                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                                                ON DUPLICATE KEY UPDATE
                                                    open_price = VALUES(open_price),
                                                    high_price = VALUES(high_price),
                                                    low_price = VALUES(low_price),
                                                    close_price = VALUES(close_price),
                                                    volume = VALUES(volume),
                                                    quote_volume = VALUES(quote_volume),
                                                    created_at = VALUES(created_at)
                                            """, (
                                                symbol,
                                                'binance_futures',
                                                timeframe,
                                                open_time_ms,
                                                close_time_ms,
                                                timestamp_dt,
                                                float(row['open']),
                                                float(row['high']),
                                                float(row['low']),
                                                float(row['close']),
                                                float(row['volume']),
                                                float(row.get('quote_volume', 0)),
                                                created_at
                                            ))
                                            timeframe_saved += 1
                                        except Exception as e:
                                            logger.error(f"ä¿å­˜åˆçº¦Kçº¿æ•°æ®å¤±è´¥ ({timeframe}): {e}")
                                            continue
                                    
                                    futures_saved += timeframe_saved
                                    logger.info(f"  âœ“ {symbol} åˆçº¦ {timeframe} Kçº¿æ•°æ®é‡‡é›†å®Œæˆï¼Œä¿å­˜ {timeframe_saved} æ¡")
                                else:
                                    logger.warning(f"  âŠ— {symbol} åˆçº¦ {timeframe}: æœªè·å–åˆ°Kçº¿æ•°æ®")
                                
                                # å»¶è¿Ÿé¿å…APIé™æµ
                                await asyncio.sleep(0.3)
                                
                            except Exception as e:
                                error_msg = f"{symbol} åˆçº¦ {timeframe}: {str(e)}"
                                errors.append(error_msg)
                                logger.error(f"é‡‡é›† {symbol} åˆçº¦ {timeframe} Kçº¿æ•°æ®å¤±è´¥: {e}")
                        
                        total_saved += futures_saved
                        if futures_saved > 0:
                            logger.info(f"{symbol} åˆçº¦æ•°æ®é‡‡é›†å®Œæˆï¼Œå…±ä¿å­˜ {futures_saved} æ¡ï¼ˆæ‰€æœ‰å‘¨æœŸï¼‰")
                        else:
                            errors.append(f"{symbol}: æ‰€æœ‰å‘¨æœŸå‡æœªè·å–åˆ°åˆçº¦æ•°æ®")
                            
                    except Exception as e:
                        error_msg = f"{symbol} åˆçº¦æ•°æ®: {str(e)}"
                        errors.append(error_msg)
                        logger.error(f"é‡‡é›† {symbol} åˆçº¦æ•°æ®å¤±è´¥: {e}")
                
            except Exception as e:
                error_msg = f"{symbol}: {str(e)}"
                errors.append(error_msg)
                logger.error(f"é‡‡é›† {symbol} æ•°æ®å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
        
        conn.commit()
        cursor.close()
        conn.close()
        
        # å¦‚æœå‹¾é€‰äº†ä¿å­˜åˆ°é…ç½®æ–‡ä»¶ï¼Œåˆ™æ›´æ–°config.yaml
        config_updated = False
        if save_to_config:
            try:
                # æ›´æ–°äº¤æ˜“å¯¹åˆ—è¡¨
                price_updated = _update_config_file(config_path, symbols, 'price', None)
                if price_updated:
                    config_updated = True
                
                # å¯¹äºKçº¿æ•°æ®ï¼Œæ›´æ–°æ‰€æœ‰æ—¶é—´å‘¨æœŸ
                if collect_kline and timeframes:
                    for tf in timeframes:
                        updated = _update_config_file(config_path, symbols, 'kline', tf)
                        if updated:
                            config_updated = True
                
                if config_updated:
                    timeframe_str = ', '.join(timeframes) if collect_kline and timeframes else ''
                    logger.info(f"é…ç½®æ–‡ä»¶å·²æ›´æ–°: æ·»åŠ äº† {len(symbols)} ä¸ªäº¤æ˜“å¯¹" + 
                              (f"ï¼Œæ—¶é—´å‘¨æœŸ {timeframe_str}" if timeframe_str else ""))
            except Exception as e:
                logger.error(f"æ›´æ–°é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
        
        return {
            'success': True,
            'total_saved': total_saved,
            'errors': errors,
            'config_updated': config_updated,
            'collect_futures': collect_futures,
            'message': f'æˆåŠŸé‡‡é›† {total_saved} æ¡æ•°æ®' + (f'ï¼Œ{len(errors)} ä¸ªé”™è¯¯' if errors else '') + 
                      (f'ï¼Œé…ç½®æ–‡ä»¶å·²æ›´æ–°' if config_updated else '')
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æ•°æ®é‡‡é›†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"æ•°æ®é‡‡é›†å¤±è´¥: {str(e)}")

